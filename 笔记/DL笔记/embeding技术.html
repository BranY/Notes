<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.2 (456745)"/><meta name="altitude" content="11"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-31 14:05:44 +0000"/><meta name="latitude" content="30.19540646839961"/><meta name="longitude" content="120.1920147976419"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-06-06 11:58:50 +0000"/><title>embeding技术</title></head><body><div><span style="font-size: 18px; color: rgb(170, 121, 66); font-weight: bold;">什么是Embedding</span><span style="font-size: 18px;">：Embedding在数学上表示一个maping, f: X -&gt; Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 &lt; X2,那么映射后在Y所属空间上同理 Y1 &lt; Y2)。那么对于word embedding</span><span style="font-size: 18px; color: rgb(255, 38, 0);">，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点</span><span style="font-size: 18px;">。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 18px;-en-paragraph:true;">word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是word representation。</span><span style="font-size: 18px;">推广开来，还有image embedding, video embedding, 都是一种将源数据映射到另外一个空间</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span>是词的一种低维向量表示，有词向量之后各种基于词向量的计算就可以做，比如相似度，出现在相同上下文的词意思应该更加接近，所有 词嵌入方法都是用数学的方法建模词与上下文的关系</span></div><div> </div><div><span style="font-size: 18px;">One-hot Encoding：（</span><font style="font-size: 14px;">解决了分类问题不好处理的属性数据；可以看作是扩充特征</font><span style="font-size: 18px;">）</span></div><ul><li><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">容易受维数灾难的困扰，尤其是将其用于 Deep Learning 的一些算法时；</span></div></li><li><div><span style="font-size: 16px; orphans: 2; widows: 2; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun;">不能很好地刻画词与词之间的相似性（术语好像叫做“词汇鸿沟”）：任意两个词之间都是孤立的。</span></div></li><li><div><span style="caret-color: rgb(79, 79, 79); font-size: 16px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">几乎不包含任何领域知识，包含的信息较少.</span></div></li><ul><li><div><span style="font-size: 16px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">方向一：训练更多的数据：题海战术。</span></div></li><li><div><span style="font-size: 16px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">方向二：加入先验知识：尽可能排除不必要的可能性。</span></div></li></ul><li><div><span style="caret-color: rgb(79, 79, 79); font-size: 16px; color: rgb(255, 38, 0); font-family: &quot;PingFang SC&quot;;">word2vec模型的输入</span></div></li><li><div><span style="font-size: 16px; color: rgb(255, 38, 0); font-family: &quot;PingFang SC&quot;;"><br/></span></div></li></ul><div><br/></div><div><br/></div><div><font style="font-size: 24px;" face="Helvetica Neue"><span style="font-size: 24px; font-family: &quot;Helvetica Neue&quot;; font-weight: bold;">Word2vec:</span></font></div><div><br/></div><div><span style="font-size: 18px;">有哪些类型的word embedding?（</span><span style="font-size: 14px;">下面两种方法</span><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">都是基于一个统计学上的假设：</span><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">经常在同一个上下文出现的单词是相似的</span><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">。只是他们的实现方式是不一样的，前者是采用词频统计，降维，矩阵分解等确定性技术；</span><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">而后者则采用了神经网络进行不确定预测，它的提出主要是采用神经网络之后计算复杂度和最终效果都比之前的模型要好</span><span style="font-size: 18px;">）</span></div><div><br style="font-size: 18px;"/></div><ul><li><div><span style="font-size: 18px;">Frequency based Embedding :</span></div></li><ul><li><div><span style="font-size: 14px;">Count vector:  </span><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">比如我们有N个文本（document），我们统计出所有文本中不同单词的数量，结果组成一个矩阵。那么每一列就是一个向量，表示这个单词在不同的文档中出现的次数</span></div></li></ul></ul><div><br style="font-size: 18px;"/></div><ul><ul><li><div><span style="font-size: 14px;">TF-IDF vector : </span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">其实就是一个权重，设想一下如果一个单词在各个文档里都出现过，那么</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">N</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">/</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">n</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">=</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">1</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">，所以</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">i</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">d</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">f</span><span style="font-size: 13.4613px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">i</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">=</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; color: rgb(255, 38, 0); font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">0</span><span style="font-size: 19.04px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-wrap: normal; word-spacing: 0px; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; top: 0px; clip: rect(1px, 1px, 1px, 1px); border: 0px !important; overflow: hidden !important; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 0;">i</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">。这就意味着这个单词并不重要</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">。这个东西其实很简单，就是在term-frequency的基础上加了一个权重，从而显著降低一些不重要/无意义的单词的frequency，比如a,an,the等。</span></div></li></ul></ul><div><br style="font-size: 18px;"/></div><ul><ul><li><div><span style="font-size: 14px;">Co-Occurrence vector：</span></div></li><ul><li><div><span style="font-size: 18px;">Co-occurrence：协同出现指的是两个单词</span><span style="font-size: 19.04px; word-wrap: normal; word-spacing: normal; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; line-height: 0;">w</span><span style="font-size: 13.4613px; word-wrap: normal; word-spacing: normal; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; line-height: 0;">1</span><span style="font-size: 18px;">和</span><span style="font-size: 19.04px; word-wrap: normal; word-spacing: normal; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; line-height: 0;">w</span><span style="font-size: 13.4613px; word-wrap: normal; word-spacing: normal; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; font-family: MJXc-TeX-main-R, MJXc-TeX-main-Rw; line-height: 0;">2</span><span style="font-size: 18px;">在一个Context Window范围内共同出现的次数</span></div></li><li><div><span style="font-size: 18px;">Context Window ：指某个单词</span><span style="font-size: 19.04px; word-wrap: normal; word-spacing: normal; white-space: pre; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; border-collapse: separate; border-spacing: 0px; box-sizing: content-box !important; font-family: MJXc-TeX-math-I, MJXc-TeX-math-Ix, MJXc-TeX-math-Iw; line-height: 0;">w</span><span style="font-size: 19.04px; word-wrap: normal; word-spacing: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; top: 0px; clip: rect(1px, 1px, 1px, 1px); border: 0px !important; overflow: hidden !important; line-height: 0;">w</span><span style="font-size: 18px;">的上下文范围的大小，也就是前后多少个单词以内的才算是上下文？比如一个Context Window Size = 2的示意图如下：</span></div></li></ul></ul></ul><div style="text-align: center;"><img src="embeding%E6%8A%80%E6%9C%AF.resources/52129ADB-6224-42F2-8F84-E20374FDB3A5.jpg" height="65" width="543"/><br/></div><ul><ul><ul><li><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: bold;">这种方法不再认为单词是独立的，而考虑了这个单词所在附近的上下文，这是一个很大的突破。</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;"> 如果两个单词经常出现在同一个上下文中，那么很可能他们有相同的含义。</span></div></li></ul></ul></ul><div><br style="font-size: 18px;"/></div><ul><ul><li><div><span style="font-size: 18px;">这</span><span style="font-size: 18px; color: rgb(255, 38, 0);">三种方法都是确定性的方法</span></div></li></ul></ul><div><br style="font-size: 18px;"/></div><ul><li><div><span style="font-size: 18px;">Prediction based Embedding</span><span style="font-size: 18px;">：是</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">一种非确定性的基于神经网络的预测模型</span><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(33, 33, 33); font-family: &quot;Roboto Condensed&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">-word2vec。它是只有一个隐含层的神经网络，且激活函数（active function）是线性的，最后一层output采用softmax来计算概率。</span></div></li></ul><div><br/></div><div><br/></div><div>
<span style="font-size: 18px; font-family: &quot;Helvetica Neue&quot;; font-stretch: normal; font-style: normal; font-variant-caps: normal; font-weight: normal; line-height: normal;">把词语映射到一个向量空间里，语义越相近的词距离越接近 （可以看作一种降维技术）</span></div><div><br/></div><div><span style="font-size: 18px; font-family: &quot;Helvetica Neue&quot;; font-stretch: normal; font-style: normal; font-variant-caps: normal; font-weight: normal; line-height: normal;">把x看作一个句子中的词，y是上下文的话，x和y放在一起时句子的概率，丢到神经网络中就是这个language model 的模型参数，应该就是这个词的向量化表示</span></div><div><br/></div><div style="text-align: center; "><img src="embeding%E6%8A%80%E6%9C%AF.resources/F3F3662E-0081-4E97-8494-9A640953F875.jpg" height="388" width="682"/><br/></div><div><span style="font-family: &quot;Helvetica Neue&quot;;">首先说明一点：</span><span style="font-family: &quot;Helvetica Neue&quot;; font-weight: bold;">隐层的激活函数其实是线性的</span><span style="font-family: &quot;Helvetica Neue&quot;;">，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用</span><span style="font-family: &quot;Helvetica Neue&quot;; font-weight: bold;">反向传播算法</span><span style="font-family: &quot;Helvetica Neue&quot;;">，本质上是</span><span style="font-family: &quot;Helvetica Neue&quot;; font-style: italic;">链式求导</span><span style="font-family: &quot;Helvetica Neue&quot;;">，在此不展开说明了，</span></div><div><span style="font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;-en-paragraph:true;">当模型训练完后，最后得到的其实是</span><span style="font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: bold;-en-paragraph:true;">神经网络的权重</span><span style="font-size: medium; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;-en-paragraph:true;">，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 Vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 Vx 就可以用来唯一表示 x。
</span></div><div><br/></div><div><font style="font-size: 24px;" face="PingFang SC" color="#4f4f4f"><span style="caret-color: rgb(79, 79, 79); font-size: 24px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">两种模型：</span></font></div><ul><li><div><font style="font-size: 18px;"><span style="font-size: 18px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">Skip-gram : 预测一个词的上下文</span></font></div></li></ul><div style="text-align: center; "><img src="embeding%E6%8A%80%E6%9C%AF.resources/CB678CF9-7860-4AA2-BA1D-8FFC74A1EA5C.jpg" height="617" width="540"/><br/></div><ul><li><div><span style="font-size: 18px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;;">CBOW（Continuous Bag of words） : 用上下文预测一个词</span></div></li></ul><div style="text-align: center; "><img src="embeding%E6%8A%80%E6%9C%AF.resources/729B0ED0-1986-440E-9D66-4183EABFF335.jpg" height="646" width="518"/><br/></div><div><br/></div><div><br/></div><ul><li><div><span style="font-size: 18px;">hierarchical softmax：(在输出层考虑了所有词)</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(85, 85, 85); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">在每次迭代中把要预测的词相关权重增加，通过归一化，同时把其他的词相关权重减少。这个不难理解，总的预测概率和是1，把其中某一个词的概率增加就意味着把其他词的预测概率打压。</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; color: rgb(85, 85, 85); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">Hierarchical Softmax 相对于原来的softmax是把一个多分类问题转换成了</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; color: rgb(85, 85, 85); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: bold;">多个二分类</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; color: rgb(85, 85, 85); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">问题。通俗地说，现在仓管人员要去仓库找一个配件，按照softmax策略是把仓库里面的配件一个个的过一遍，最后确定下来是哪个，Hierarchical Softmax则是根据预先记录的表知道了要找的配件在第二个房间的第一个货架第二行第一个直接去取的。</span></span></div></li><ul style="padding: 0px;"><li style="list-style: none;"><div><span style="font-size: 18px;">本质是把 N 分类问题变成 log(N)次二分类</span></div></li></ul><li><div><span style="font-size: 18px;">negative sampling</span></div></li><ul style="padding: 0px;"><li style="list-style: none;"><div><span style="font-size: 18px;">本质是预测总体类别的一个子集</span></div></li></ul></ul><div><br/></div><div><span style="font-size: 18px;">参数：</span></div><ul><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;"> sentences：可以是一个list，对于大语料集，建议使用BrownCorpus,Text8Corpus或·ineSentence构建。</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;"> sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法。</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">size：是指特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百。</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">window：表示当前词与预测词在一个句子中的最大距离是多少</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">alpha: 是学习速率</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">seed：用于随机数发生器。与初始化词向量有关。</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5</span></div></li><li><div><span style="caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;;">迭代次数</span></div></li><li><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal;">sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)</span></div></li></ul><div><br/></div><div><span style="font-size: 19px; font-family: &quot;Helvetica Neue&quot;;">参考链接：</span></div><ul><li><div><a style="font-size: 14px;" href="https://zhuanlan.zhihu.com/p/26306795">词向量word2vec的本质</a></div></li><li><div><a href="https://www.zhihu.com/question/32275069">有谁可以解释下word embedding?（知乎）</a></div></li><li><div><a href="https://blog.csdn.net/mytestmy/article/details/26961315">深度学习word2vec基础</a></div></li><li><div><font style="font-size: 14px;"><span style="orphans: 2; widows: 2;"><a style="font-size: 14px; font-family: -apple-system, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, &quot;Source Han Sans SC&quot;, &quot;Noto Sans CJK SC&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif;" href="https://www.zhihu.com/question/53011711">word2vec 相比之前的 Word Embedding 方法好在什么地方？</a></span></font></div></li><li><div><a href="https://kexue.fm/usr/uploads/2017/04/2833204610.pdf">word2vec中的数学</a></div></li><li><div><a href="https://kexue.fm/usr/uploads/2017/04/146269300.pdf">深度学习之实战word2vec</a></div></li><li><div><a href="https://www.zybuluo.com/evilking/note/871932">word2vec基于负采样的模型</a></div></li><li><div><a href="http://yobobobo001.github.io/2016/05/31/%E6%88%91%E6%89%80%E7%90%86%E8%A7%A3%E7%9A%84word2vec/">我所理解的word2vec</a></div></li><li><div><a href="https://www.fanyeong.com/2017/10/10/word2vec/">word2vec初探</a></div></li><li><div><a href="https://ronxin.github.io/wevi/">Word2vec可视化</a></div></li></ul><div><br/></div></body></html>