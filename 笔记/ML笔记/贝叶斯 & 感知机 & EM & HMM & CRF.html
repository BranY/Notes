<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.38462257385254"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:47:42 +0000"/><meta name="latitude" content="30.19547523312549"/><meta name="longitude" content="120.1919652606613"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-06-04 14:21:46 +0000"/><title>贝叶斯 &amp; 感知机 &amp; EM &amp; HMM &amp; CRF</title></head><body><div><br/></div><div><img src="%E8%B4%9D%E5%8F%B6%E6%96%AF%20&amp;%20%E6%84%9F%E7%9F%A5%E6%9C%BA%20&amp;%20EM%20&amp;%20HMM%20&amp;%20CRF.resources/F95EFD45-ED39-4913-8150-A6E59B7E5B5D.png" height="342" width="1388"/><br/></div><div><br/></div><div>1.朴素贝叶斯和LR的区别是啥</div><ul><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><span style="background-color: rgb(255, 250, 165); font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;-evernote-highlight:true;">朴素贝叶斯是生成模型</span><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">，根据已有样本进行贝叶斯估计</span><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">学习出先验概率</span><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">P(Y)和</span><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">条件概率</span><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">P(X|Y)，进而求出联合分布概率P(XY),最后利用贝叶斯定理求解P(Y|X)， 而</span><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">LR是判别模型，根据极大化对数似然函数直接求出条件概率</span><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">P(Y|X)；</span></font></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">朴素贝叶斯是基于很强的条件独立假设（在已知分类Y的条件下，各个特征变量取值是相互独立的），而LR则对此没有要求；</span></font></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">朴素贝叶斯适用于数据集少的情景，而LR适用于大规模数据集。</span></font></span></div></li></ul><div><span style="caret-color: rgb(79, 79, 79); color: rgb(79, 79, 79);">朴素贝叶斯方法的优缺点：</span></div><ul><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">对小规模的数据表现很好，适合多分类任务，适合增量式训练。</span></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">样本容量增加时，收敛更快；隐变量存在时也可适用。</span></span></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">对输入数据的表达形式很敏感（离散、连续等)</span></span></div></li><li><div><span style="caret-color: rgb(79, 79, 79); font-size: 14px; color: rgb(79, 79, 79);">由于样本属性独立性假设，如果属性存在很强关联效果不好</span></div></li><li><div><span style="caret-color: rgb(79, 79, 79); font-size: 14px; color: rgb(79, 79, 79);">文本分类，垃圾邮件的分类</span></div></li><li><div><span style="caret-color: rgb(79, 79, 79); font-size: 14px; color: rgb(79, 79, 79);">多项式模型，计算先验概率和条件概率，平滑，但特征是连续性变量，多项式模型就会产生很多零概率，即使使用平滑，所得到的条件概率也很难接近真实情况; 高斯模型</span></div></li></ul><div><br/></div><div><span style="box-sizing: border-box;outline: 0px;word-break: break-all;letter-spacing: normal;orphans: 2;text-indent: 0px;text-transform: none;white-space: normal;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;-en-paragraph:true;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;">LR 不关心样本中类别的比例及类别下出现特征的概率，它直接给出预测模型的式子。设每个特征都有一个权重，训练样本数据更新权重w，得出最终表达式。梯度法。</span></span></div><ul><li><div><span style="font-size: 14px; orphans: 2; text-align: justify; widows: 2; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; line-height: 26px; text-decoration: underline;-en-paragraph:true;">优点：</span><span style="font-size: 14px; orphans: 2; text-align: justify; widows: 2; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">直接预测往往准确率更高；简化问题；可以反应数据的分布情况，类别的差异特征；适用于较多类别的识别。</span></div></li><li><div><span style="font-size: 14px; outline: 0px; word-break: break-all; orphans: 2; widows: 2; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; line-height: 26px; text-decoration: underline;-en-paragraph:true;">缺点：</span><span style="font-size: 14px; outline: 0px; word-break: break-all; orphans: 2; widows: 2; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">收敛慢；不适用于有隐变量的情况。</span></div></li></ul><div><br/></div><div>2.朴素贝叶斯的朴素体现在哪里</div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal;">利用贝叶斯定理求解联合概率P(XY)时，需要计算条件概率P(X|Y)。在计算P(X|Y)时，朴素贝叶斯做了一个很强的条件独立假设（</span><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(255, 38, 0); font-variant-caps: normal; font-variant-ligatures: normal;">当Y确定时，X的各个分量取值之间相互独立</span><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal;">），即P(X1=x1,X2=x2,...Xj=xj|Y=yk) = P(X1=x1|Y=yk)*P(X2=x2|Y=yk)*...*P(Xj=xj|Y=yk)</span></font></span></div><div><br/></div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">3.朴素贝叶斯在估计条件概率P(X|Y)时出现0概率怎么办？能怎么办，当然是引入平滑啊</span></span></div><div><br/></div><div>4.贝叶斯分类器与贝叶斯学习的不同 ？<span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="background-color: rgb(255, 250, 165); font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;-evernote-highlight:true;">通过最大后验概率进行单点估计；后者：进行分布估计</span><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">。</span></span></div><div><br/></div><div>5.为什么属性独立性假设在实际任务中很难成立，但是朴素贝叶斯依然可以取的很好的效果？</div><ul><li><div><span style="box-sizing: border-box; outline: 0px; font-size: 14px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">对于分类任务来说</span><span style="box-sizing: border-box;outline: 0px;font-size: 14px;word-break: break-all;letter-spacing: normal;orphans: 2;text-indent: 0px;text-transform: none;white-space: normal;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;background-color: rgb(255, 255, 255);-en-paragraph:true;"><span style="background-color: rgb(255, 255, 255); font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;">，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类</span></span><span style="box-sizing: border-box; outline: 0px; font-size: 14px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">； </span></div></li><li><div><span style="font-size: 14px; orphans: 2; widows: 2;"><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun;">如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消</span></span><span style="font-size: 14px; orphans: 2; widows: 2; color: rgb(79, 79, 79); font-family: &quot;PingFang SC&quot;, &quot;Microsoft YaHei&quot;, SimHei, Arial, SimSun;">，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响。</span></div></li></ul><div><br/></div><div><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;;">EM算法：</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">有时候任务中含有一些不能观察到的隐含变量，样本的产生和隐含变量有关，而求模型的参数时一般用最大似然估计，由于隐变量的存在，所以对似然函数参数求导是求不出来的，这时采用</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">EM算法来求导。</span></div><div><span style="font-size: 14px; orphans: 2; text-align: justify; widows: 2; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; font-weight: bold; line-height: 26px;-en-paragraph:true;">总结</span><span style="font-size: 14px; orphans: 2; text-align: justify; widows: 2; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">：是一种迭代算法，用于含有隐变量的概率模型参数的极大似然估计。两个步骤交替计算：</span></div><ul><li><div style="box-sizing: border-box; outline: 0px; padding: 0px; text-align: justify; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">E步：利用当前估计的参数值，求出在该参数下隐含变量的条件概率值（计算</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px; text-decoration: underline;-en-paragraph:true;">对数似然的期望值）</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">； </span></font></div></li><li><div style="box-sizing: border-box; outline: 0px; padding: 0px; text-align: justify; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><span style="font-size: 14px; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数的最大值（寻找能使E步产生的似然期望最大化的</span><span style="font-size: 14px; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px; text-decoration: underline;-en-paragraph:true;">参数值</span><span style="font-size: 14px; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">。）然后，新得到的参数值重新被用于</span><span style="font-size: 14px; outline: 0px; word-break: break-all; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">E步.....直到收敛到局部最优解。（note：每次迭代实际在求Q函数及其极大，即每次迭代使似然函数增大或达到局部极值。）</span></div></li></ul><div style="box-sizing: border-box; outline: 0px; padding: 0px; text-align: justify; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">通俗来讲：就是对于一个含有隐变量的概率模型，目标是极大化观测数据</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(79, 79, 79); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">Y关于参数theta的对数似然函数。
</span></font></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; text-align: justify; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); margin-top: 1em; margin-bottom: 1em;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(223, 64, 42); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">优点：简单性和普适性，可看作是一种非梯度优化方法（解决梯度下降等优化方法的缺陷：求和的项数将随</span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(223, 64, 42); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">  </span><span style="box-sizing: border-box; outline: 0px; word-break: break-all; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(223, 64, 42); font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;-en-paragraph:true;">着隐变量的数目以指数级上升，会给梯度计算带来麻烦）</span></font></div><div><span style="box-sizing: border-box;outline: 0px;word-break: break-all;letter-spacing: normal;orphans: 2;text-indent: 0px;text-transform: none;white-space: normal;widows: 2;word-spacing: 0px;-webkit-text-stroke-width: 0px;-en-paragraph:true;"><span style="font-size: 14px; color: rgb(223, 64, 42); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 26px;">缺点：对初始值敏感，不同的初值可能得到不同的参数估计值；不能保证找到全局最优值。</span></span></div><div><br/></div><div><br/></div><div>（1）EM算法 </div><div><br/></div><div>　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 </div><div><br/></div><div>　　注意：<font color="#ff2600">EM算法是对初值敏感的</font>，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，<font color="#ff2600">也就是说<span style="font-weight: bold; box-sizing: border-box; outline: 0px; word-break: break-all;">EM算法不能</span>保证找到全局最优值</font>。对于EM的导出方法也应该掌握。 </div><div><br/></div><div>（2）HMM算法 </div><div><br/></div><div>　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（<span style="box-sizing: border-box; outline: 0px; line-height: normal; font-size: 19.2px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; word-break: break-all; clip: rect(1.93em, 1000.58em, 2.659em, -999.997em); top: -2.497em; font-family: STIXGeneral-Italic;">π</span>，A，B）：初始<span style="font-weight: bold; box-sizing: border-box; outline: 0px; word-break: break-all;">状态</span>概率向量<span style="box-sizing: border-box; outline: 0px; line-height: normal; font-size: 19.2px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; word-break: break-all; clip: rect(1.93em, 1000.58em, 2.659em, -999.997em); top: -2.497em; font-family: STIXGeneral-Italic;">π</span>，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。 </div><div>马尔科夫三个基本问题：</div><p/><ul><li><div style="box-sizing: border-box; outline: 0px; padding: 0px; list-style-type: disc; word-break: break-all;"><span style="box-sizing: border-box; outline: 0px; word-break: break-all;">概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法</span></div></li><li><div style="box-sizing: border-box; outline: 0px; padding: 0px; list-style-type: disc; word-break: break-all;"><span style="box-sizing: border-box; outline: 0px; word-break: break-all;"/>学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。</div></li><li><div style="box-sizing: border-box; outline: 0px; padding: 0px; list-style-type: disc; word-break: break-all;">预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）</div></li></ul><p style="box-sizing: border-box; outline: 0px; padding: 0px; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; text-align: justify; word-break: break-all; font-family: -apple-system, &quot;SF UI Text&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif, SimHei, SimSun; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"/><div>（3）条件随机场CRF </div><div><br/></div><div>　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 </div><div><br/></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">　　之所以总把HMM和CRF进行比较，<font color="#ff2600">主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）</font>。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。
</span></div><p/><p style="box-sizing: border-box; outline: 0px; padding: 0px; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; text-align: justify; word-break: break-all; font-family: -apple-system, &quot;SF UI Text&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif, SimHei, SimSun; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"/><div>（4）HMM和CRF对比 </div><div>　　<font color="#ff2600">其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</font><br/></div><div/><div><br/></div><div><br/></div><div><font style="font-size: 18px;">参考链接</font>：</div><ul><li><div><a href="https://blog.csdn.net/jingyi130705008/article/details/79464740">贝叶斯面试总结1</a></div></li><li><div><a href="https://blog.csdn.net/qq_34896915/article/details/75040686">贝叶斯面试总结2</a></div></li><li><div><a href="https://blog.csdn.net/qq_34896915/article/details/75040578">EM算法面试总结</a> （结合词对齐算法）</div></li><li><div><a href="http://nlp.ict.ac.cn/~liuqun/course/MachineTranslation/2010ict/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E5%8E%9F%E7%90%86%E4%B8%8E%E6%96%B9%E6%B3%95%E8%AE%B2%E4%B9%89(03)%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%96%B9%E6%B3%95.pdf">基于词的统计机器翻译方法</a></div></li><li><div><a href="http://nlp.ict.ac.cn/~liuqun/course/MachineTranslation/">中科院计算所-机器翻译原理与方法</a></div></li><li><div><a href="http://saliormoon.github.io/2016/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%981/">机器学习面试问题</a></div></li></ul><div><br/></div></body></html>