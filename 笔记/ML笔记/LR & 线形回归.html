<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.22901821136475"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:36:49 +0000"/><meta name="latitude" content="30.19541223010093"/><meta name="longitude" content="120.1919994048567"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-06-05 14:11:26 +0000"/><title>LR &amp; 线形回归</title></head><body><div>LR假设数据服从伯努利分布，然后通过最大似然估计方法，运用梯度下降来求解参数，来达到分类的目的；</div><ul><li><div><span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; orphans: 2; widows: 2;"><font color="#ff2600">逻辑回归的假设：</font><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的</span><span style="font-weight: bold; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">第一个</span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">基本假设是</span><span style="font-weight: bold; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">假设数据服从伯努利分布。</span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是</span><span style="color: rgb(0, 0, 0); font-family: STIXGeneral-Italic; font-size: 17.64px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: normal; -webkit-text-stroke-width: 0px; line-height: normal; word-wrap: normal; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; clip: rect(1.874em, 1000.46em, 2.894em, -999.997em); top: -2.491em;">p</span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);">,抛中为负面的概率是</span><span style="color: rgb(0, 0, 0); font-family: STIXGeneral-Regular; font-size: 17.64px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: normal; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); line-height: normal; word-wrap: normal; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; clip: rect(1.647em, 1002.04em, 2.894em, -999.997em); top: -2.491em;">1</span><span style="color: rgb(0, 0, 0); font-family: STIXGeneral-Regular; font-size: 17.64px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: normal; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); line-height: normal; word-wrap: normal; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; clip: rect(1.647em, 1002.04em, 2.894em, -999.997em); top: -2.491em;">−</span><span style="color: rgb(0, 0, 0); font-family: STIXGeneral-Italic; font-size: 17.64px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: nowrap; widows: 2; word-spacing: normal; -webkit-text-stroke-width: 0px; line-height: normal; word-wrap: normal; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; clip: rect(1.647em, 1002.04em, 2.894em, -999.997em); top: -2.491em;">p</span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">.</span></span></div></li><li><div><span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; orphans: 2; widows: 2;"><font color="#ff2600">逻辑回归的损失函数</font>：极大似然函数（对数损失函数）</span></div></li><li><div><span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; orphans: 2; widows: 2;"><font color="#ff2600">逻辑回归的求解方法</font>：</span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">由于该极大似然函数无法直接求解，一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。</span></div></li><ul><li style=" padding: 0px; list-style: circle;"><div><span style="font-family: Verdana;"><b>批梯度下降会获得全局最优解</b>，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</span></div></li><li style=" padding: 0px; list-style: circle;"><div><span style="font-family: Verdana;"><b>随机梯度下降是以高方差频繁更新</b>，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。</span></div></li><li><div><span style="font-family: Verdana;">小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</span></div></li><li><div><span style="font-family: Verdana;"><font color="#ff2600">第一个是如何对模型选择合适的学习率</font>。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。</span></div></li><li><div><span style="font-family: Verdana;">第二个是如何对参数选择合适的学习率。在实践中，<font color="#ff2600">对每个参数都保持的同样的学习率也是很不合理的</font>。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。</span></div></li></ul><li><div><span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; orphans: 2; widows: 2;">逻辑回归的目的以及如何分类：二分类，大于阈值划分</span></div></li></ul><div><span style="font-family: Verdana;"><br/></span></div><div><font face="Verdana">LR的优缺点：</font></div><div><font face="Verdana">优点：</font></div><ul><li><div><font face="Verdana">模型形式简单，可解释很好。</font></div></li><li><div><font face="Verdana">模型在很多场景效果还行，在工程上经常作为baseline，如果特征工程做的，效果不会太差，特征工程可以并发地做；</font></div></li><li><div><font style="font-family: Verdana;">适合离散性特征， <span style="font-family: Verdana;">w.x 矩阵乘法，可以很容易的并行化， 特征的维度就可以很高。高维稀疏特征在训练和预测时占用的内存也很小</span></font></div></li><li><div><font face="Verdana">得分输出在【0-1】之间，具有概率特性，很适合点击率预估、推荐和广告等场景；</font></div></li></ul><div><font face="Verdana">缺点：</font></div><ul><li><div><font face="Verdana">因为简单，模型效果不一定是最好的，很难去拟合数据的真实分布</font></div></li><li><div><font face="Verdana">无法处理非线形问题</font></div></li><li><div><font face="Verdana">特征工程<span style="color: rgb(66, 99, 6); font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: 0.544px; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">需要人工进行转换为线性特征，十分消耗人力，并且质量不能保证。考虑到特征交叉，只能两两交叉， 100维度的话就有1我的特征，复杂度很高。三个以上的特征交叉机会是不可行的</span></font></div></li></ul><ul><li><div><font face="Verdana">很难处理数据不平衡问题</font></div></li></ul><div><span style="font-family: Verdana;"><br/></span></div><div>虽然他是线形的，他也可以用到一些Mixed LR。<span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; orphans: 2; widows: 2;"><font color="#ff2600">逻辑回归本身无法筛选特征。有时候，会用gbdt来筛选特征，然后再上逻辑回归</font>。</span></div><div><br/></div><div><br/></div><div><span style="font-weight: bold; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</span></div><ul><li><div><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。</span><span style="font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font color="#ff2600">将极大似然函数取对数以后等同于对数损失函数</font></span><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。梯度更新时<span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</span></span></div></li><li><div><div><span style="font-family: Verdana;"><font color="#ff2600">为什么不选平方损失函数的呢</font>？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</span></div></div><div><span style="font-family: Verdana;"><br/></span></div></li></ul><ul style="padding: 0px; word-break: break-all; color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><li style=" padding: 0px; list-style: disc;"><div><span style="background-color: rgb(153, 204, 255); font-weight: bold;">逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</span></div></li></ul><ul style="padding: 0px; word-break: break-all; color: rgb(0, 0, 0); font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255);"><ul><li style="font-family: Verdana, Arial, Helvetica, sans-serif; padding: 0px; list-style: disc;"><div><span style="font-family: Verdana;">如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</span></div></li><li style="font-family: Verdana, Arial, Helvetica, sans-serif; padding: 0px; list-style: disc;"><div><span style="font-family: Verdana;">但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练完后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。</span></div></li><li style="font-family: Verdana, Arial, Helvetica, sans-serif; padding: 0px; list-style: disc;"><div><span style="font-family: Verdana;">如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</span></div></li><li><div><font face="Verdana">实际中将高度相关的特征去掉是因为：</font></div></li><ul><li><div><font face="Verdana"/><span style="font-family: Verdana; background-color: transparent;">去掉高度相关的特征会让模型的可解释性更好</span></div></li><li><div><span style="color: rgb(0, 0, 0); font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。</span></div></li></ul></ul></ul><div/><div><span style="font-family: Verdana;"><b><br/></b></span></div><div/><div>1.  LR与线性回归的区别与联系</div><div><br/></div><div>逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</div><div><br/></div><div>逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。LR的损失函数不能使用平方损失函数（原因见李宏毅)</div><div><br/></div><div>2. LR为何要对特征离散化</div><ul><li><div>离散特征的增加和减少都很容易，易于模型的快速迭代；</div></li><li><div> 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</div></li><li><div>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</div></li><li><div>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</div></li><li><div> 离散化后可以进行<span style="color: rgb(255, 38, 0);">特征交叉</span>，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</div></li><li><div>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</div></li></ul><div><br/></div><div>3. 对于维度极低的特征，选择线性还是非线性分类器？非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分</div><ul><li><div>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</div></li><li><div>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</div></li><li><div>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</div></li></ul><div><br/></div><div>4. SVM、LR、决策树的对比。</div><ul><li><div>模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝</div></li><li><div>损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失</div></li><li><div>数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感</div></li><li><div> 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核</div></li></ul><div><br/></div><div>5.LR与SVM的区别</div><ul><li><div>LR和SVM都可以处理分类问题，且一般用于处理线性二分类问题（在改进的情况下可以处理多分类问题） </div></li><li><div>两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。</div></li></ul><div><span style="font-weight: bold;"> 区别</span>： </div><ul><li><div>从目标函数来看，于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</div></li><li><div>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</div></li><li><div>逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</div></li><li><div>LR 能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。</div></li></ul><div><br/></div><div>6.lasso回归就是加了l1, ridge或者岭回归就是加了l2</div><div><br/></div><div>7.多分类怎么做？<font face="Helvetica Neue" style="font-size: 14px;"><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">如果</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.942em, 1000.46em, 2.968em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">y</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">不是在[0,1]中取值，而是在</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1000.74em, 2.74em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">K</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1000.74em, 2.74em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">K</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1000.74em, 2.74em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">K</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">个类别是互斥的，即 </span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1002em, 2.968em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">y</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1002em, 2.968em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">=</span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.771em, 1002em, 2.968em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">i</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;"> 的时候意味着 </span><span style="text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: 0px; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; clip: rect(1.942em, 1000.46em, 2.968em, -999.997em); top: -2.563em; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal; line-height: normal;">y</span><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-family: &quot;Helvetica Neue&quot;; font-size: 14px; color: rgb(102, 102, 102); font-variant-caps: normal; font-variant-ligatures: normal;">不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。</span></font></div><div><br/></div><div>参考链接：</div><ol><li><div>Regression研究：<a href="https://cloud.tencent.com/developer/article/1004813">https://cloud.tencent.com/developer/article/1004813</a></div></li><li><div>李宏毅课程地址：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></div></li><li><div>回归案例研究：   <a href="https://blog.csdn.net/zyq522376829/article/details/66577532">https://blog.csdn.net/zyq522376829/article/details/66577532</a></div></li></ol><div><br/></div></body></html>