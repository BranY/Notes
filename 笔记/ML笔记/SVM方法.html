<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.30877685546875"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:45:07 +0000"/><meta name="latitude" content="30.19547056861554"/><meta name="longitude" content="120.1919491298685"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-06-04 14:19:19 +0000"/><title>SVM方法</title></head><body><div>SVM的优点：</div><div><br/></div><ul><li><div>可以解决小样本情况下的机器学习问题。</div></li><li><div>可以提高泛化性能。</div></li><li><div>可以解决高维数据问题。</div></li><li><div>可以解决非线性问题。</div></li><li><div>可以避免神经网络结构选择和局部极小点问题。</div></li></ul><div><br/></div><div>SVM的缺点：</div><ul><li><div>对非线性问题没有通用解决方案，必须谨慎选择Kernelfunction来处理，核函数的高纬映射能力解释不强，kernel也是尽可能去估计数据的原有分布，因此存在一个强假设，如果数据每个维度都高度不一致，这样的分布是很难试出来的</div></li><li><div>对缺失数据敏感(缺失某些特征数据，向量数据不完整，SVM希望样本在特征空间-希尔伯特空间线性可分，所以特征空间的好坏对svm很重要，缺失特征将影响训练结果的好坏</div></li></ul><div>二分类问题适应较好</div><div><br/></div><div>特征维度很高时选择非线性分类器，特征维度很低时选择线形分类器</div><div><br/></div><div><span style="font-size: 18px;">1.如何组织训练数据：</span></div><ul><li><div>将数据分开为训练集、测试集；</div></li><li><div>对训练集寻优，构建好决策函数，训练分类器；</div></li><li><div>用测试集验证准确率，达到满足需要的准确率后，就可以用新数据分类。</div></li></ul><div><br/></div><div><span style="font-size: 18px;">2.如何调节惩罚因子</span></div><div><br/></div><div>惩罚因子（参数C）：为了使用松弛变量才引入的，表示对离群点的重视程度。C越大越重视，越不想丢掉离群点。可用来解决数据集偏斜问题，方法是调整惩罚因子，给样本数量少的类更大的惩罚因子。</div><div><br/></div><div>惩罚因子C决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的C越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把C定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">惩罚因子C不是一个变量，整个优化问题在解的时候，C是一个你必须事先指定的值</span>，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个C的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C一直是定值，要记住。</div><div><br/></div><div><span style="font-size: 18px;">3.如何防止过拟合</span></div><div><br/></div><div>过拟合表现为在训练数据上模型的预测很准，在未知数据上预测很差。过拟合主要是因为训练数据中的异常点，这些点严重偏离正常位置。我们知道，决定SVM最优分类超平面的恰恰是那些占少数的支持向量，如果支持向量中碰巧存在异常点，那么我们傻傻地让SVM去拟合这样的数据，最后的超平面就不是最优的。</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">解决过拟合的办法是为SVM引入了松弛变量ξ。</span>综上，可以得出：为了解决离群的异常点引入了松弛变量，为了解决松弛变量的引入带来的分类精度的影响问题，引入了惩罚因子，松弛变量用来舍弃离群点，而惩罚因子用来保护离群点。</div><div><br/></div><div>除却svm，一般防止模型过拟合，提高模型泛化能力时，最常用的方法是：正则化，即在对模型的目标函数或代价函数加上正则项。此外，为了防止过拟合，我们也会用到一些其他方法，如：early stopping、数据集扩增（Data augmentation）、Dropout等。</div><div><br/></div><div><span style="font-size: 18px;">4.SVM原理是什么？为什么要间隔最大化</span></div><div><br/></div><div>SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）</div><ul><li><div>训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</div></li><li><div>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</div></li><li><div>当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</div></li></ul><div><br/></div><div>注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）—学习的对偶问题—软间隔最大化（引入松弛变量）—非线性支持向量机（核技巧）。</div><div><br/></div><ul><li><div>当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。</div></li><li><div>线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。</div></li></ul><div><br/></div><div><span style="font-size: 18px;">5.为什么要转换为对偶问题来求解</span></div><ul><li><div>是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）</div></li></ul><div><br/></div><ul><li><div>自然引入核函数，进而推广到非线性分类问题。</div></li></ul><div><br/></div><div><br/></div><div><span style="font-size: 18px;">6.为啥要引入核函数，都有哪些核函数</span></div><div><br/></div><div><span style="font-family: -apple-system, &quot;SF UI Text&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif, SimHei, SimSun; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font color="#ff2600">核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积</font></span></div><div /><div>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。</div><div><span style="color: rgb(79, 79, 79); font-family: -apple-system, &quot;SF UI Text&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif, SimHei, SimSun; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"> </span></div><div>核函数的定义：K(x,y)=&lt;ϕ(x),ϕ(y)&gt;K(x,y)=&lt;ϕ(x),ϕ(y)&gt;，即在特征空间的内积等于它们在原始样本空间中通过核函数K计算的结果。</div><div><br/></div><div>除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</div><div><br/></div><div><br/></div><div><span style="font-size: 18px;">7.Sklearn/libsvm中的SVM都有什么参数可以调节？</span></div><div style="text-align: center; "><div><img src="SVM%E6%96%B9%E6%B3%95.resources/56058CC2-ECDA-4FA6-99C0-8C4A4080977F.png" height="496" width="690"/><br/></div></div><div><br/></div><div>9.应用场景</div><div>二分类</div><div><span style="font-size: 18px;">参考链接：</span></div><ul><li><div><span style="font-size: 14px;"><a style="font-size: 14px;" href="https://blog.csdn.net/v_july_v/article/details/7624837">理解SVM的三层境界</a></span></div></li><li><div><a href="https://blog.csdn.net/cheese_pop/article/details/79916557">SVM面试题</a></div></li><li><div><a href="https://blog.csdn.net/qll125596718/article/details/6910921">SVM松弛变量和惩罚因子</a></div></li><li><div><a href="https://blog.csdn.net/vincent2610/article/details/52033250">SVM如何避免过拟合</a></div></li><li><div><a href="https://blog.csdn.net/heyongluoyao8/article/details/49429629">避免过拟合的方法</a></div></li></ul><div><br/></div></body></html>