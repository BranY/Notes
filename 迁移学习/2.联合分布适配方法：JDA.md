---
typora-copy-images-to: ../picture
---

# 联合分布适配法-JDA

联合分布适配法（joint distribution adaptation, JDA）解决的也是迁移学习中一类很大的问题：domain adaptation。**如何用有标注的源域数据![(\mathcal{D}_s=\{\mathbf{x}_s,P(\mathbf{x}_s)\})](http://www.zhihu.com/equation?tex=%28%5Cmathcal%7BD%7D_s%3D%5C%7B%5Cmathbf%7Bx%7D_s%2CP%28%5Cmathbf%7Bx%7D_s%29%5C%7D%29)来标定完全无标注的目标域![(\mathcal{D}_t=\{\mathbf{x}_t,P(\mathbf{x}_t)\})](http://www.zhihu.com/equation?tex=%28%5Cmathcal%7BD%7D_t%3D%5C%7B%5Cmathbf%7Bx%7D_t%2CP%28%5Cmathbf%7Bx%7D_t%29%5C%7D%29)**

JDA是一个概率分布适配的方法，而且适配的是联合概率。先来简单普及一下知识：边缘概率、条件概率和联合概率。对于一个随机变量![X](http://www.zhihu.com/equation?tex=X)，![x \in X](http://www.zhihu.com/equation?tex=x+%5Cin+X)是它的元素，对于每一个元素，都对应一个类别![y \in Y](http://www.zhihu.com/equation?tex=y+%5Cin+Y)。那么，它的边缘概率为![P(X)](http://www.zhihu.com/equation?tex=P%28X%29),条件概率为![P(y|X)](http://www.zhihu.com/equation?tex=P%28y%7CX%29)，联合概率为![p(X,y)](http://www.zhihu.com/equation?tex=p%28X%2Cy%29)。**JDA方法就是要适配源域和目标域的联合概率**。

## JDA方法

假设是最基本的出发点。JDA的假设有两点：**1）源域和目标域边缘分布不同，2）源域和目标域条件分布不同。**那么目标就是同时适配两个分布，于是作者很自然地提出了联合分布适配方法：**适配联合概率**。

那么，**JDA方法的目标就是，寻找一个变换![\mathbf{A}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D)，使得经过变换后的![P(\mathbf{A}^\top \mathbf{x}_s)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_s%29) 和 ![P(\mathbf{A}^\top \mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_t%29)的距离能够尽可能地接近，同时，![P(y_s|\mathbf{A}^\top \mathbf{x}_s)](http://www.zhihu.com/equation?tex=P%28y_s%7C%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_s%29)和![P(y_t|\mathbf{A}^\top \mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28y_t%7C%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_t%29)的距离也要小**。

### 边缘分布适配

首先来适配边缘分布，也就是![P(\mathbf{A}^\top \mathbf{x}_s)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_s%29)和 ![P(\mathbf{A}^\top \mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_t%29)的距离能够尽可能地接近。其实这个操作就是迁移成分分析（TCA）。这里仍然使用MMD距离来最小化源域和目标域的最大均值差异。MMD距离是

![\left \Vert \frac{1}{n} \sum_{i=1}^{n} \mathbf{A}^\top \mathbf{x}_{s_i} - \frac{1}{m} \sum_{i=1}^{m} \mathbf{A}^\top \mathbf{x}_{t_i} \right \Vert ^2_\mathcal{H}](http://www.zhihu.com/equation?tex=%5Cleft+%5CVert+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_%7Bs_i%7D+-+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7D+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_%7Bt_i%7D+%5Cright+%5CVert+%5E2_%5Cmathcal%7BH%7D)

这个式子实在不好求解，引入核方法，就变成：

![D(\mathcal{D}_s,\mathcal{D}_t)=tr(\mathbf{A}^\top \mathbf{X} \mathbf{M}_0 \mathbf{X}^\top \mathbf{A})](http://www.zhihu.com/equation?tex=D%28%5Cmathcal%7BD%7D_s%2C%5Cmathcal%7BD%7D_t%29%3Dtr%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BM%7D_0+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%29)

其中![\mathbf{A}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D)就是变换矩阵，我们把它加黑加粗，![\mathbf{X}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D)是源域和目标域合并起来的数据。![\mathbf{M}_0](http://www.zhihu.com/equation?tex=%5Cmathbf%7BM%7D_0)是一个MMD矩阵：

![(\mathbf{M}_0)_{ij}=\begin{cases} \frac{1}{n^2}, & \mathbf{x}_i,\mathbf{x}_j \in \mathcal{D}_s\\ \frac{1}{m^2}, & \mathbf{x}_i,\mathbf{x}_j \in \mathcal{D}_t\\ -\frac{1}{mn}, & \text{otherwise} \end{cases}](http://www.zhihu.com/equation?tex=%28%5Cmathbf%7BM%7D_0%29_%7Bij%7D%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7Bn%5E2%7D%2C+%26+%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D_s%5C%5C+%5Cfrac%7B1%7D%7Bm%5E2%7D%2C+%26+%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D_t%5C%5C+-%5Cfrac%7B1%7D%7Bmn%7D%2C+%26+%5Ctext%7Botherwise%7D+%5Cend%7Bcases%7D)

![n,m](http://www.zhihu.com/equation?tex=n%2Cm)分别是源域和目标域样本的个数。

### 条件分布适配

第二个目标要做的目标是：适配源域和目标域的条件概率分布。也就是需要找一个变换![\mathbf{A}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D)，使得![P(y_s|\mathbf{A}^\top \mathbf{x}_s)](http://www.zhihu.com/equation?tex=P%28y_s%7C%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_s%29)和![P(y_t|\mathbf{A}^\top \mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28y_t%7C%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_t%29)的距离也要小。**再用一遍MMD， 可是目标域里，没有![y_t](http://www.zhihu.com/equation?tex=y_t)，没法求目标域的条件分布！**

直接去建模![P(y_t|\mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28y_t%7C%5Cmathbf%7Bx%7D_t%29)不行。那么，能不能有别的办法可以逼近这个条件概率？类条件概率![\mathbf{x}_t|y_t](http://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_t%7Cy_t)中， 根据贝叶斯公式![P(y_t|\mathbf{x}_t)=p(y_t)p(\mathbf{x}_t|y_t)](http://www.zhihu.com/equation?tex=P%28y_t%7C%5Cmathbf%7Bx%7D_t%29%3Dp%28y_t%29p%28%5Cmathbf%7Bx%7D_t%7Cy_t%29)，如果忽略![P(\mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bx%7D_t%29)，就可以用![P(\mathbf{x}_t|y_t)](http://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bx%7D_t%7Cy_t%29)来近似![P(y_t|\mathbf{x}_t)](http://www.zhihu.com/equation?tex=P%28y_t%7C%5Cmathbf%7Bx%7D_t%29)？在统计学上，有一个东西叫做**充分统计量**，大概意思就是说，**如果样本里有太多的东西未知，样本足够好，我们就能够从中选择一些统计量，近似地代替我们要估计的分布**。

实际怎么做呢？这里依然没有![y_t](http://www.zhihu.com/equation?tex=y_t)。采用的方法是，用![(\mathbf{x}_s,y_s)](http://www.zhihu.com/equation?tex=%28%5Cmathbf%7Bx%7D_s%2Cy_s%29)来训练一个简单的分类器（比如knn、逻辑斯特回归），到![\mathbf{x}_t](http://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D_t)上直接进行预测， 得到一些伪标签![\hat{y}_t](http://www.zhihu.com/equation?tex=%5Chat%7By%7D_t)，根据伪标签来计算，这个问题就可解了。

类与类之间的MMD距离表示为

![\sum_{c=1}^{C}\left \Vert \frac{1}{n_c} \sum_{\mathbf{x}_{s_i} \in \mathcal{D}^{(c)}_s} \mathbf{A}^\top \mathbf{x}_{s_i} - \frac{1}{m_c} \sum_{\mathbf{x}_{t_i} \in \mathcal{D}^{(c)}_t} \mathbf{A}^\top \mathbf{x}_{t_i} \right \Vert ^2_\mathcal{H}](http://www.zhihu.com/equation?tex=%5Csum_%7Bc%3D1%7D%5E%7BC%7D%5Cleft+%5CVert+%5Cfrac%7B1%7D%7Bn_c%7D+%5Csum_%7B%5Cmathbf%7Bx%7D_%7Bs_i%7D+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_s%7D+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_%7Bs_i%7D+-+%5Cfrac%7B1%7D%7Bm_c%7D+%5Csum_%7B%5Cmathbf%7Bx%7D_%7Bt_i%7D+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_t%7D+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7Bx%7D_%7Bt_i%7D+%5Cright+%5CVert+%5E2_%5Cmathcal%7BH%7D)

其中，![n_c,m_c](http://www.zhihu.com/equation?tex=n_c%2Cm_c)分别标识源域和目标域中来自第c类的样本个数。同样地用核方法，得到了下面的式子

![\sum_{c=1}^{C}tr(\mathbf{A}^\top \mathbf{X} \mathbf{M}_c \mathbf{X}^\top \mathbf{A})](http://www.zhihu.com/equation?tex=%5Csum_%7Bc%3D1%7D%5E%7BC%7Dtr%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BM%7D_c+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%29)

其中![\mathbf{M}_c](http://www.zhihu.com/equation?tex=%5Cmathbf%7BM%7D_c)为

![(\mathbf{M}_c)_{ij}=\begin{cases} \frac{1}{n^2_c}, & \mathbf{x}_i,\mathbf{x}_j \in \mathcal{D}^{(c)}_s\\ \frac{1}{m^2_c}, & \mathbf{x}_i,\mathbf{x}_j \in \mathcal{D}^{(c)}_t\\ -\frac{1}{m_c n_c}, & \begin{cases} \mathbf{x}_i \in \mathcal{D}^{(c)}_s ,\mathbf{x}_j \in \mathcal{D}^{(c)}_t \\ \mathbf{x}_i \in \mathcal{D}^{(c)}_t ,\mathbf{x}_j \in \mathcal{D}^{(c)}_s \end{cases}\\ 0, & \text{otherwise}\end{cases}](http://www.zhihu.com/equation?tex=%28%5Cmathbf%7BM%7D_c%29_%7Bij%7D%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7Bn%5E2_c%7D%2C+%26+%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_s%5C%5C+%5Cfrac%7B1%7D%7Bm%5E2_c%7D%2C+%26+%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_t%5C%5C+-%5Cfrac%7B1%7D%7Bm_c+n_c%7D%2C+%26+%5Cbegin%7Bcases%7D+%5Cmathbf%7Bx%7D_i+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_s+%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_t+%5C%5C+%5Cmathbf%7Bx%7D_i+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_t+%2C%5Cmathbf%7Bx%7D_j+%5Cin+%5Cmathcal%7BD%7D%5E%7B%28c%29%7D_s+%5Cend%7Bcases%7D%5C%5C+0%2C+%26+%5Ctext%7Botherwise%7D%5Cend%7Bcases%7D)

### 学习策略

把上述两个距离结合起来，得到了一个总的优化目标：

![\min \sum_{c=0}^{C}tr(\mathbf{A}^\top \mathbf{X} \mathbf{M}_c \mathbf{X}^\top \mathbf{A}) + \lambda \Vert \mathbf{A} \Vert ^2_F](http://www.zhihu.com/equation?tex=%5Cmin+%5Csum_%7Bc%3D0%7D%5E%7BC%7Dtr%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BM%7D_c+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%29+%2B+%5Clambda+%5CVert+%5Cmathbf%7BA%7D+%5CVert+%5E2_F)

通过![c=0 \cdots C](http://www.zhihu.com/equation?tex=c%3D0+%5Ccdots+C)就把两个距离统一起来了！其中的![\lambda \Vert \mathbf{A} \Vert ^2_F](http://www.zhihu.com/equation?tex=%5Clambda+%5CVert+%5Cmathbf%7BA%7D+%5CVert+%5E2_F)是正则项，使得模型泛化能力好。

这里缺一个限制条件，**和TCA一样变换前后数据的方差要维持不变**。怎么求数据的方差呢还和TCA一样：![\mathbf{A}^\top \mathbf{X} \mathbf{H} \mathbf{X}^\top \mathbf{A} = \mathbf{I}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BH%7D+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D+%3D+%5Cmathbf%7BI%7D)，其中的![\mathbf{H}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BH%7D)也是中心矩阵，![\mathbf{I}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BI%7D)是单位矩阵。也就是说，又添加了一个新的优化目标是要![\max \mathbf{A}^\top \mathbf{X} \mathbf{H} \mathbf{X}^\top \mathbf{A}](http://www.zhihu.com/equation?tex=%5Cmax+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BH%7D+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D)（这一个步骤等价于PCA）。和原来的优化目标合并，统一写一下优化目标

![\min \frac{\sum_{c=0}^{C}tr(\mathbf{A}^\top \mathbf{X} \mathbf{M}_c \mathbf{X}^\top \mathbf{A}) + \lambda \Vert \mathbf{A}}{ \mathbf{A}^\top \mathbf{X} \mathbf{H} \mathbf{X}^\top \mathbf{A}}](http://www.zhihu.com/equation?tex=%5Cmin+%5Cfrac%7B%5Csum_%7Bc%3D0%7D%5E%7BC%7Dtr%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BM%7D_c+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%29+%2B+%5Clambda+%5CVert+%5Cmathbf%7BA%7D%7D%7B+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BH%7D+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%7D)

这个式子实在不好求解。有个东西叫做rayleigh quotient，上面两个一样的这种形式。因为![\mathbf{A}](http://www.zhihu.com/equation?tex=%5Cmathbf%7BA%7D)是可以进行拉伸而不改改变最终结果的，而如果下面为0的话，整个式子就求不出来值了。所以，直接就可以让下面不变，只求上面。所以我们最终的优化问题形式变成：

![ \min \quad \sum_{c=0}^{C}tr(\mathbf{A}^\top \mathbf{X} \mathbf{M}_c \mathbf{X}^\top \mathbf{A}) + \lambda \Vert \mathbf{A} \Vert ^2_F \quad \text{s.t.} \quad \mathbf{A}^\top \mathbf{X} \mathbf{H} \mathbf{X}^\top \mathbf{A} = \mathbf{I}](http://www.zhihu.com/equation?tex=+%5Cmin+%5Cquad+%5Csum_%7Bc%3D0%7D%5E%7BC%7Dtr%28%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BM%7D_c+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D%29+%2B+%5Clambda+%5CVert+%5Cmathbf%7BA%7D+%5CVert+%5E2_F+%5Cquad+%5Ctext%7Bs.t.%7D+%5Cquad+%5Cmathbf%7BA%7D%5E%5Ctop+%5Cmathbf%7BX%7D+%5Cmathbf%7BH%7D+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D+%3D+%5Cmathbf%7BI%7D)

应用用拉格朗日法嘛，最后变成：

![\left(\mathbf{X} \sum_{c=0}^{C} \mathbf{M}_c \mathbf{X}^\top + \lambda \mathbf{I}\right) \mathbf{A} =\mathbf{X} \mathbf{H} \mathbf{X}^\top \mathbf{A} \Phi ](http://www.zhihu.com/equation?tex=%5Cleft%28%5Cmathbf%7BX%7D+%5Csum_%7Bc%3D0%7D%5E%7BC%7D+%5Cmathbf%7BM%7D_c+%5Cmathbf%7BX%7D%5E%5Ctop+%2B+%5Clambda+%5Cmathbf%7BI%7D%5Cright%29+%5Cmathbf%7BA%7D+%3D%5Cmathbf%7BX%7D+%5Cmathbf%7BH%7D+%5Cmathbf%7BX%7D%5E%5Ctop+%5Cmathbf%7BA%7D+%5CPhi+)

其中的 ![\Phi](http://www.zhihu.com/equation?tex=%5CPhi) 是拉格朗日乘子。可是伪标签终究是伪标签啊，肯定精度不高，怎么办？可以通过多次迭代。后一次做的时候，用上一轮得到的标签来作伪标签。这样的目的是得到越来越好的伪标签，而参与迁移的数据是不会变的。这样往返多次，结果就自然而然好了。

JDA方法比较巧妙，同时适配两个分布，然后非常精巧地规到了一个优化目标里。用弱分类器迭代，最后达到了很好的效果，和TCA的主要区别有两点：1）TCA是无监督的（边缘分布适配不需要label），JDA需要源域有label；2）TCA不需要迭代，JDA需要迭代



## 参考链接

[1. 迁移学习github](https://github.com/jindongwang/transferlearning)

[2.Domain adaptation via tranfer component analysis](https://mega.nz/#!JTwElLrL!j5-TanhHCMESsGBNvY6I_hX6uspsrTxyopw8bPQ2azU)

[3.知乎专栏-联合分布适配法](https://zhuanlan.zhihu.com/p/27336930)

[4. Transfer Feature Learning with Joint Distribution Adaptation](http://ise.thss.tsinghua.edu.cn/~mlong/doc/joint-distribution-adaptation-iccv13.pdf)

[5.王进东知乎专栏-迁移学习](https://zhuanlan.zhihu.com/wjdml)

