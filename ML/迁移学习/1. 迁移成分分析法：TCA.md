---
typora-copy-images-to: ../picture
---

# 迁移成分分析法-TCA

​       TCA方法属于迁移学习中的特征迁移，如下图：

![9AD1074C-418E-4A28-891B-30F415ECCD1B](/Users/yangwenjia/Notes/ML/picture/9AD1074C-418E-4A28-891B-30F415ECCD1B.png)

### 背景介绍

机器学习中有一类非常有效的方法叫做降维（dimensionality reduction），用简单的话来说就是，**把原来很高维度的数据（比如数据有1000多列）用很少的一些代表性维度来表示（比如1000多维用100维来表示）而不丢失关键的数据信息**。这些降维方法多种多样，比如：主成分分析（PCA，principal component analysis）、局部线性嵌入（LLE,locally linear embedding）、拉普拉斯特征映射（Laplacian eigen-map）等。这些方法的过程大体都是一个大的矩阵作为输入，然后输出一个小矩阵。那么在迁移学习中，也可通过降维来达到数据维度减少，达到迁移学习目的呢，就是要说的迁移成分分析（TCA，transfer component analysis）。

### TCA的本质是什么

TCA属于**基于特征的迁移学习方法**。跟PCA很像：PCA是一个大矩阵进去，一个小矩阵出来，TCA呢，是两个大矩阵进去，两个小矩阵出来。从学术角度讲，**TCA针对domain adaptation问题中，源域和目标域处于不同数据分布时，将两个领域的数据一起映射到一个高维的再生核希尔伯特空间。在此空间中，最小化源和目标的数据距离，同时最大程度地保留它们各自的内部属性**。直观地理解就是，在现在这个维度上不好最小化它们的距离，那么我就找个映射，在映射后的空间上让它们最接近，那么我不就可以进行分类了吗？TCA本质是完成迁移学习的要求。迁移学习的要求是**让源域和目标域距离尽可能小呗**。

### TCA的假设

TCA的假设是： 源域和目标域的边缘分布是不一样的，也就是说，![P(X_S) \ne P(X_T)](http://www.zhihu.com/equation?tex=P%28X_S%29+%5Cne+P%28X_T%29)，所以不能直接用传统的机器学习方法。但是呢，**TCA假设存在一个特征映射$\phi$，使得映射后数据的分布![P(\phi(X_S)) \approx P(\phi(X_T))](http://www.zhihu.com/equation?tex=P%28%5Cphi%28X_S%29%29+%5Capprox+P%28%5Cphi%28X_T%29%29)，更进一步条件分布![P(Y_S | \phi(X_S)) \approx P(Y_T | \phi(X_T))](http://www.zhihu.com/equation?tex=P%28Y_S+%7C+%5Cphi%28X_S%29%29+%5Capprox+P%28Y_T+%7C+%5Cphi%28X_T%29%29)**。现在的目标是，找到这个合适的$\phi$映射。

### 寻找特征变换的映射

实际中存在无穷个这样的![\phi](http://www.zhihu.com/equation?tex=%5Cphi)，肯定不能通过穷举的方法来找![\phi](http://www.zhihu.com/equation?tex=%5Cphi)的。那么怎么办呢？回到迁移学习的本质上来：**最小化源域和目标域的距离**。如果先假设这个![\phi](http://www.zhihu.com/equation?tex=%5Cphi)是已知的，然后去求距离，看看能推出什么呢？

距离怎么算？从欧氏距离到马氏距离，从曼哈顿距离到余弦相似度，什么距离合适呢？TCA利用了一个经典的也算是比较“高端”的距离叫做**最大均值差异**（MMD，maximum mean discrepancy）。这个距离的公式如下：

![dist(X'_{src},X'_{tar})= \begin{Vmatrix} \frac{1}{n_1} \sum \limits_{i=1}^{n_1} \phi(x_{src_i}) - \frac{1}{n_2}\sum \limits _{i=1}^{n_2} \phi(x_{tar_i}) \end{Vmatrix}_{\mathcal{H}}](http://www.zhihu.com/equation?tex=dist%28X%27_%7Bsrc%7D%2CX%27_%7Btar%7D%29%3D+%5Cbegin%7BVmatrix%7D+%5Cfrac%7B1%7D%7Bn_1%7D+%5Csum+%5Climits_%7Bi%3D1%7D%5E%7Bn_1%7D+%5Cphi%28x_%7Bsrc_i%7D%29+-+%5Cfrac%7B1%7D%7Bn_2%7D%5Csum+%5Climits+_%7Bi%3D1%7D%5E%7Bn_2%7D+%5Cphi%28x_%7Btar_i%7D%29+%5Cend%7BVmatrix%7D_%7B%5Cmathcal%7BH%7D%7D)



MMD就是求映射后源域和目标域的均值之差。但是目前想求的![\phi](http://www.zhihu.com/equation?tex=%5Cphi)仍然没法求。TCA是怎么做的呢，这里就要感谢矩阵了！我们发现，**上面这个MMD距离平方展开后，有二次项乘积的部分**！那么，联系在SVM中学过的核函数，**把一个难求的映射用核函数的形式来求**，TCA引入了一个核矩阵![K](http://www.zhihu.com/equation?tex=K)：

![K=\begin{bmatrix}K_{src,src} & K_{src,tar}\\K_{tar,src} & K_{tar,tar}\end{bmatrix} ](http://www.zhihu.com/equation?tex=K%3D%5Cbegin%7Bbmatrix%7DK_%7Bsrc%2Csrc%7D+%26+K_%7Bsrc%2Ctar%7D%5C%5CK_%7Btar%2Csrc%7D+%26+K_%7Btar%2Ctar%7D%5Cend%7Bbmatrix%7D+)

以及![L](http://www.zhihu.com/equation?tex=L):

![L_{ij}=\begin{cases} \frac{1}{{n_1}^2} & x_i,x_j \in X_{src},\\ \frac{1}{{n_2}^2} & x_i,x_j \in X_{tar},\\ -\frac{1}{n_1 n_2} & \text{otherwise} \end{cases}](http://www.zhihu.com/equation?tex=L_%7Bij%7D%3D%5Cbegin%7Bcases%7D+%5Cfrac%7B1%7D%7B%7Bn_1%7D%5E2%7D+%26+x_i%2Cx_j+%5Cin+X_%7Bsrc%7D%2C%5C%5C+%5Cfrac%7B1%7D%7B%7Bn_2%7D%5E2%7D+%26+x_i%2Cx_j+%5Cin+X_%7Btar%7D%2C%5C%5C+-%5Cfrac%7B1%7D%7Bn_1+n_2%7D+%26+%5Ctext%7Botherwise%7D+%5Cend%7Bcases%7D)

这样的好处是，直接把那个难求的距离，变换成了下面的形式：

![trace(KL)-\lambda trace(K)](http://www.zhihu.com/equation?tex=trace%28KL%29-%5Clambda+trace%28K%29)

trace是矩阵的迹，用人话来说就是一个矩阵对角线元素的和。这样是不是感觉离目标又进了一步呢？

其实这个问题到这里就已经是可解的了，也就是说，属于计算机的部分已经做完了。只不过它是一个数学中的**半定规划（SDP，semi-definite programming）的问题**，解决起来非常耗费时间。由于TCA的第一作者用更简单的方法来解决。如下：![\widetilde{K}=({K}{K}^{-1/2}\widetilde{W})(\widetilde{W}^{\top}{K}^{-1/2}{K})={K}WW^{\top}{K}](http://www.zhihu.com/equation?tex=%5Cwidetilde%7BK%7D%3D%28%7BK%7D%7BK%7D%5E%7B-1%2F2%7D%5Cwidetilde%7BW%7D%29%28%5Cwidetilde%7BW%7D%5E%7B%5Ctop%7D%7BK%7D%5E%7B-1%2F2%7D%7BK%7D%29%3D%7BK%7DWW%5E%7B%5Ctop%7D%7BK%7D)

这里的W矩阵是比K更低维度的矩阵。最后的W就是问题的解答了！

### 求解映射

TCA最后的优化目标是：

![\begin{split} \min_W \quad& \text{tr}(W^\top KLKW) + \mu \text{tr}(W^\top W)\\ \text{s.t.} \quad & W^\top KHKW = I_m \end{split} ](http://www.zhihu.com/equation?tex=%5Cbegin%7Bsplit%7D+%5Cmin_W+%5Cquad%26+%5Ctext%7Btr%7D%28W%5E%5Ctop+KLKW%29+%2B+%5Cmu+%5Ctext%7Btr%7D%28W%5E%5Ctop+W%29%5C%5C+%5Ctext%7Bs.t.%7D+%5Cquad+%26+W%5E%5Ctop+KHKW+%3D+I_m+%5Cend%7Bsplit%7D+)

这里的$H$是一个中心矩阵，![H = I_{n_1 + n_2} - 1/(n_1 + n_2)\mathbf{11}^\top](http://www.zhihu.com/equation?tex=H+%3D+I_%7Bn_1+%2B+n_2%7D+-+1%2F%28n_1+%2B+n_2%29%5Cmathbf%7B11%7D%5E%5Ctop).

这个式子下面的条件是什么意思呢？那个min的目标就是要最小化源域和目标域的距离，加上W的约束让它不能太复杂。那么下面的条件是什么呢？下面的条件就是要实现第二个目标：维持各自的数据特征。TCA要维持的是什么特征呢？文章中说是variance，但是实际是scatter matrix，就是数据的散度。就是说，一个矩阵散度怎么计算？对于一个矩阵![A ](http://www.zhihu.com/equation?tex=A+)，它的scatter matrix就是![AHA^\top](http://www.zhihu.com/equation?tex=AHA%5E%5Ctop)。这个![H](http://www.zhihu.com/equation?tex=H)就是上面的中心矩阵啦。

### 小结

TCA方法的步骤。输入是两个特征矩阵，首先计算L和H矩阵，然后选择一些常用的核函数进行映射（比如线性核、高斯核）计算K，接着求![({K}L{K}+\mu I)^{-1}{K}H{K}](http://www.zhihu.com/equation?tex=%28%7BK%7DL%7BK%7D%2B%5Cmu+I%29%5E%7B-1%7D%7BK%7DH%7BK%7D)的前m个特征值。然后，得到的就是源域和目标域的降维后的数据，最后用传统机器学习方法了。

TCA最核心工作是什么呢？一是把问题转化成数学问题转化得很彻底；二是最优化求解方法很厉害。

## 参考链接

[1. 迁移学习github](https://github.com/jindongwang/transferlearning)

[2.Domain adaptation via tranfer component analysis](https://mega.nz/#!JTwElLrL!j5-TanhHCMESsGBNvY6I_hX6uspsrTxyopw8bPQ2azU)

