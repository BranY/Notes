<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.38462257385254"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:46:07 +0000"/><meta name="latitude" content="30.19547523312549"/><meta name="longitude" content="120.1919652606613"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-05-21 07:19:07 +0000"/><title>常见数据 &amp; 特征处理方法</title></head><body><div style="text-align: center; "><div><img src="%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%20&amp;%20%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95.resources/DD1B7CEA-C8E8-4C65-A0DB-43705B3F137A.jpg" height="744" width="1682"/><br/></div></div><div><span style="font-size: 18px;">PCA原理：</span></div><ul><li><div>通过投影将n维数据映射到k维空间，在设定原有保留数据结构信息（95%）的条件下，通过trial &amp; error筛选出一个合适的K值，来实现最大化降低维度和最大化保留原有数据的结构信息</div></li><li><div>通过比较不同的线之间的最优距离，选取投影距离只和最短的线作为K=1的最优情况</div></li><li><div>K个特征向量组成的矩阵，对这个矩阵转置对原数据变形，得到的结果就是降维后的数据</div></li></ul><div><br/></div><div>PCA一般不被推荐来避免过拟合</div><ul><li><div>从过拟合的角度来看，定义中数据内置了Y值，Y值的监督让我门看到training loss与validation loss的差异，如果差异巨大就存在过拟合，所以没有Y值就不存在过拟合</div></li><li><div>从PCA的角度来看，PCA中只考虑X值，没有考虑Y值，虽然降低维度后，仍然保持X极高比例的方差信息，但没有任何依据说新生产的数据能提炼任何与Y值相关的信息</div></li></ul><div><br/></div><div><span style="font-size: 18px;">特征选择的方法论：</span></div><div><br/></div><ul><li><div>计算每一个特征与响应变量的相关性：<span style="color: rgb(255, 38, 0);">工程上常用的手段有计算皮尔逊系数和互信息系数</span>，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；</div></li><li><div>构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；</div></li><li><div><span style="color: rgb(255, 38, 0);">通过L1正则项来选择特征</span>：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；</div></li><li><div>训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；</div></li><li><div><span style="color: rgb(255, 38, 0);">通过特征组合后再来选择特征</span>：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。</div></li><li><div>通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。</div></li><li><div>GBDT可以自动选择重要的特征</div></li><li><div>PCA也可以看作是在进行特征选择</div></li></ul><div><br/></div><div><br/></div><div><span style="font-size: 18px;">如何进行特征选择？</span></div><div>特征选择是一个重要的数据预处理过程，主要有两个原因：</div><ul><li><div>减少特征数量、降维，使模型泛化能力更强，减少过拟合;</div></li><li><div>增强对特征和特征值之间的理解</div></li></ul><div><br/></div><div> 常见的特征选择方式：</div><ul><li><div>去除方差较小的特征</div></li><li><div>正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零</div></li><li><div> 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题: 重要的特征有可能得分很低（关联特征问题），这种方法对特征变量类别多的特征越有利（偏向问题）。</div></li><li><div>稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。</div></li></ul><div><br/></div><div><span style="font-size: 18px;">数据预处理</span></div><div><br/></div><div>1. 缺失值，填充缺失值fill\, NA：</div><div>        - 离散：None</div><div>        - 连续：均值，等</div><div>        - 缺失值太多，则直接去除该列</div><div>2. 连续值：离散化。有的模型（如决策树）需要离散值</div><div>3. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。</div><div>4. 皮尔逊相关系数，去除高度相关的列</div><div><br/></div><div><span style="font-size: 18px;">数据分布不平衡的解决方法：</span></div><ul><li><div>采样，对小样本加噪声采样(增加小类样本的个数)，对大样本进行下采样,减少大样本的个数</div></li><li><div>进行特殊的加权，如在Adaboost中或者SVM中</div></li><li><div>采用Bagging方法，因为有放回抽样，boosting对样本有加权也可以</div></li><li><div>考虑数据的先验分布，样本权重也可以作为参数</div></li><li><div>采用对不平衡数据集不敏感的算法</div></li><li><div>改变评价标准：用AUC/ROC来进行评价</div></li></ul><div><br/></div><div><span style="font-size: 18px;">为何要经常对数据做归一化</span></div><ul><li><div>归一化为什么能提高梯度下降法求解最优解的速度？</div></li></ul><div><br/></div><div>如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；</div><div>而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</div><div><br/></div><div>因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</div><div><br/></div><ul><li><div>有可能提高精度：一些分类器需要计算样本之间的距离（如欧氏距离），如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。</div></li></ul><div><br/></div><ul><li><div>概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。</div></li></ul><div><br/></div><div><a href="https://blog.csdn.net/JNingWei/article/details/78866591">深度学习中的归一化</a></div><div><br/></div><div><span style="font-size: 18px;">常用归一化方法：</span></div><ul><li><div>线性归一化 ：这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。</div></li><li><div> 标准差归一化</div></li></ul><div><br/></div><div>数据标准化和归一化的区别 ：标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。</div><div><br/></div><div>特征向量的缺失值处理</div><ul><li><div> 缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。</div></li><li><div>缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:</div></li><li><div>把NaN直接作为一个特征，假设用0表示</div></li><li><div>用均值填充；</div></li><li><div>用随机森林等算法预测填充</div></li></ul><div><br/></div><div><span style="font-size: 18px;">什么是共线性, 跟过拟合有什么关联?</span></div><div><br/></div><div>共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。</div><div><br/></div><div>共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。</div><div><br/></div><div>特征向量归一化</div><ul><li><div>线性函数转换</div></li><li><div>对数函数转换</div></li><li><div>反余切函数转换</div></li><li><div>减去均值，除以方差:</div></li></ul><div><br/></div><div><span style="font-size: 18px;">参考链接：</span></div><ul><li><div>机器学习中的数据清洗与特征处理综述 ：<a href="https://tech.meituan.com/machinelearning-data-feature-process.html">https://tech.meituan.com/machinelearning-data-feature-process.html</a></div></li><li><div>SNE与t-SNE降维算法理解： <a href="http://qiancy.com/2016/11/12/sne-tsne/">http://qiancy.com/2016/11/12/sne-tsne/</a></div></li></ul><div><br/></div></body></html>