<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.22901821136475"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:36:49 +0000"/><meta name="latitude" content="30.19541223010093"/><meta name="longitude" content="120.1919994048567"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-05-21 04:36:13 +0000"/><title>LR &amp; 线形回归</title></head><body><div>lr的原理就是一个伯努利分布，然后通过最大似然估计，似然函数去做的学习，简单适合很多离散性特征，可以很快并行实现， w.x  分布式矩阵乘法， 特征维度就可以非常高。 得分就是0-1的数值， 具有概率特性，比如在点击率预估，推荐、广告里面就很容易用到。缺点就是模型简单（线形），效果不一定是最好的</div><div><br/></div><div>因为简单成为广告推荐等的默认算法</div><div><br/></div><div><br/></div><div>1.  LR与线性回归的区别与联系</div><div><br/></div><div>逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。</div><div><br/></div><div>逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。LR的损失函数不能使用平方损失函数（原因见李宏毅)</div><div><br/></div><div>2. LR为何要对特征离散化</div><ul><li><div>离散特征的增加和减少都很容易，易于模型的快速迭代；</div></li><li><div> 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</div></li><li><div>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</div></li><li><div>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</div></li><li><div> 离散化后可以进行<span style="color: rgb(255, 38, 0);">特征交叉</span>，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</div></li><li><div>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当</div></li><li><div>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</div></li></ul><div><br/></div><div>3. 对于维度极低的特征，选择线性还是非线性分类器？非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分</div><ul><li><div>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</div></li><li><div>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</div></li><li><div>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</div></li></ul><div><br/></div><div>4. SVM、LR、决策树的对比。</div><ul><li><div>模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝</div></li><li><div>损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失</div></li><li><div>数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感</div></li><li><div> 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核</div></li></ul><div><br/></div><div>5.LR与SVM的区别</div><ul><li><div>LR和SVM都可以处理分类问题，且一般用于处理线性二分类问题（在改进的情况下可以处理多分类问题） </div></li><li><div>两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。</div></li></ul><div><span style="font-weight: bold;"> 区别</span>： </div><ul><li><div>从目标函数来看，于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</div></li><li><div>SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(250, 250, 250);"><span style="background-color: rgb(250, 250, 250); font-size: 14px; color: rgb(65, 63, 63); font-family: &quot;Helvetica Neue&quot;, Helvetica, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">逻辑回归通过非线性映射,大大减小了离分类平面较远的点的权重,相对提升了与分类最相关的数据点的权重</span></span></div></li><li><div>逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</div></li><li><div>LR 能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。</div></li></ul><div><br/></div><div>6.lasso回归就是加了l1, ridge或者岭回归就是加了l2</div><div><br/></div><div>参考链接：</div><ol><li><div>Regression研究：<a href="https://cloud.tencent.com/developer/article/1004813">https://cloud.tencent.com/developer/article/1004813</a></div></li><li><div>李宏毅课程地址：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></div></li><li><div>回归案例研究：   <a href="https://blog.csdn.net/zyq522376829/article/details/66577532">https://blog.csdn.net/zyq522376829/article/details/66577532</a></div></li><li><div><br/></div></li></ol><div><br/></div></body></html>