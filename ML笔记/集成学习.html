<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11.38462257385254"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-19 12:46:26 +0000"/><meta name="latitude" content="30.19547523312549"/><meta name="longitude" content="120.1919652606613"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-05-23 18:03:00 +0000"/><title>集成学习</title></head><body><div>个体学习器：要有一定的准确性，不能太坏，并且要有多样性；</div><div><br/></div><div>存在两种：个体学习器存在强依赖关系、必须串行生成的序列化算法（boosting) ; 个体学习器不存在依强依赖关系，可同时生成的并行化方法（bagging）。</div><div><br/></div><div>Boosting : 西瓜书 P173-177</div><div><br/></div><div>Bagging :  西瓜书 P178-180</div><div><br/></div><div><span style="font-weight: bold;">从偏差方差角度，Boosting 主要关注降低偏差（bias）, 因此Boosting 能基于泛化性能弱的学习器得到强集成；Bagging 主要关注降低方差（variance），因此其在不剪枝的决策树、神经网络等易受样本干扰的学习器上效用更好</span>；</div><div><br/></div><div><span style="text-decoration: underline;">有关方差-偏差分析见P44-46</span></div><div><br/></div><div><span style="font-size: 18px;">GBDT (Gradient Boosting Decison Tree) :</span></div><ul><li><div>残差版本： 残差其实就是真实值和预测值之间的差值，在学习的过程中，首先学习一颗回归树，然后将“真实值-预测值”得到残差，再把残差作为一个学习目标，学习下一棵回归树，依次类推，直到残差小于某个接近0的阀值或回归树数目达到某一阀值。其核心思想是每轮通过拟合残差来降低损失函数。 第一棵树是正常的，之后所有的树的决策全是由残差来决定。</div></li><li><div> 梯度版本：与残差版本把GBDT说成一个残差迭代树，认为每一棵回归树都在学习前N-1棵树的残差不同。Gradient版本把GBDT说成一个梯度迭代树，使用梯度下降法求解，认为每一棵回归树在学习前N-1棵树的梯度下降值。</div></li></ul><div>总的来说两者相同: 都是迭代回归树，都是累加每颗树结果作为最终结果（Multiple Additive Regression Tree)，每棵树都在学习前N-1棵树尚存的不足，从总体流程和输入输出上两者是没有区别的；</div><ul><li><div>两者的不同主要在于每步迭代时，是否使用Gradient作为求解方法。前者不用Gradient而是用残差—-残差是全局最优值，Gradient是局部最优方向步长，即前者每一步都在试图让结果变成最好，后者则每步试图让结果更好一点。</div></li><li><div>看起来前者更科学一点–有绝对最优方向不学，为什么舍近求远去估计一个局部最优方向呢？原因在于灵活性。前者最大问题是，由于依赖残差，cost function一般固定为反映残差的均方差，因此很难处理纯回归问题之外的问题。而后者求解方法为梯度下降，只要可求导的cost function都可以使用。</div></li></ul><div><br/></div><div>从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。</div><div><br/></div><div>为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，用的是类别的预测概率值和真实概率值的差来拟合损失。</div><div><br/></div><div><span style="font-size: 18px; font-weight: bold;">GBDT正则化：</span></div><ul><li><div>和Adaboost类似的正则化项，即步长(learning rate)。定义为\nu,对于前面的弱学习器的迭代f_{k}(x) = f_{k-1}(x) + h_k(x)</div></li><li><div>如果加上了正则化项，则有f_{k}(x) = f_{k-1}(x) + \nu h_k(x)  \nu的取值范围为0 &lt; \nu \leq 1 。对于同样的训练集学习效果，较小的\nu意味着我们需要更多的弱学习器的迭代次数。通常用步长和迭代最大次数一起来决定算法的拟合效果。</div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">SGBT：</span>正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，<span style="color: rgb(255, 38, 0);">随机森林使用的是放回抽样，而这里是不放回抽样</span>。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。</div></li><li><div>使用子采样的GBDT有时也称作随机梯度提升树由(Stochastic Gradient Boosting Tree, SGBT) 。<span style="color: rgb(255, 38, 0);">由于使用子采样，可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</span></div></li><li><div>是对于弱学习器即CART回归树进行正则化剪枝</div></li></ul><div><br/></div><div><span style="font-size: 18px;">常见问题：</span></div><div>1.XGBoost与GBDT 的区别</div><ul><li><div>GBDT</div></li><ul><li><div>非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换</div></li><li><div>缺点也很明显，Boost 是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">高维稀疏特征</span></div></li><li><div>传统 GBDT 在优化时只用到一阶导数信息</div></li></ul><li><div>XGBoost</div></li><ul><li><div>公式推导中用到了<span style="color: rgb(255, 38, 0);">二阶导数</span>，用了二阶泰勒展开。（ xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得二阶倒数形式）</div></li><li><div>显示的把树模型复杂度作为正则项加到优化目标中, <span style="color: rgb(255, 38, 0);">对树的结构进行了正则化约束</span>，防止模型过度复杂，降低了过拟合的可能性</div></li><li><div>节点分裂的方式不同，<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">gbdt是用的gini系数，xgboost是经过优化推导后的</span></div></li><li><div><span style="color: rgb(255, 38, 0);">Shrinkage（缩减</span>），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。</div></li><li><div>如何寻找特征：<span style="color: rgb(255, 38, 0);">xgboost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性. </span>xgboost利用梯度优化模型算法</div></li><li><div><span style="font-weight: bold;">样本是不放回的</span>(一个样本连续重复抽出,梯度来回踏步会不会高兴).  xgboost支持子采样, 也就是每轮计算可以不使用全部样本。</div></li><li><div>实现了分裂点寻找近似算法, 利用了特征的稀疏性</div></li><li><div>可以在不选定损失函数具体形式的情况下用于算法优化分析, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性</div></li></ul></ul><div><br/></div><div>2.AdaBoost与GBDT的异同</div><ul><li><div>Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）</div></li><li><div>GBDT与AdaBoost相同的地方在于要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来</div></li></ul><div>不同在于：</div><ul><li><div>AdaBoost通过每个分类器的分类结果改变每个样本的权值用于新的分类器和生成权值，但不改变每个样本</div></li><li><div>GBDT将每个分类器对样本的预测值与真实值的差值（或者梯度差值）传入下一个分类器来生成新的分类器和权值(这个差值就是下降方向)，而每个样本的权值不变</div></li></ul><div><br/></div><div>3.Bagging为何难以提升朴素贝叶斯分类器的性能</div><div>   Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。</div><div><br/></div><div>4.<span style="color: rgb(148, 33, 146);">随机森林为何比决策树Bagging(GBDT)集成的训练速度更快</span></div><div>   随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快，多棵树独立，可以并行训练</span>。(随机森林中的随机体现在两方面：一是有放回的抽取数据，而是随机选择一部分特征训练)</div><div><span style="caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51);"><br/></span></div><div><span style="caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51);">GBDT和随机森林有什么区别？相同点是两个都是集成学习，一个是bagging（每棵树是独立的，权重一样，用的是投票表决机制）， 一个是boosting（没颗树是有依赖的，它的权重是不一样的，最后用的加权累积， boosting可以看作是一种加法模型）. 再展开讲。实际验证效果中GBDT是最好的，工业验证过了（很多比赛），比如xgboost， lightGBM</span></div><div>相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。</div><div>不同点：</div><ul><li><div>组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成</div></li><li><div>组成随机森林的树可以并行生成，而GBDT是串行生成</div></li><li><div>随机森林的结果是<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">多数表决的</span>，而GBDT则是多棵树累加之和</div></li><li><div>随机森林是减少模型的方差，而GBDT是减少模型的偏差</div></li></ul><div><br/></div><div>RF的特征选择的核心思想是random test,做法是对某个特征，如果用另外一个随机值替代它之后效果变差则说明特征比较重要，所占权重应该较大! 随机值如何选择呢？一是使用均匀或者高斯分布抽取随机值来替换，二是通过排列组合的方式将N个样本的第i个特征值重新打乱进行分布，一般用这种方法，保证了特征替代值与原特征分布近似，只是重排列，</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">GBDT的优缺点</span></div><div>优点有：</div><ul><li><div>可以灵活处理各种类型的数据，包括连续值和离散值</div></li></ul><ul><li><div>可以选择出重要的特征（<span style="color: rgb(255, 38, 0);">xgboost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性.）</span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(79, 79, 79); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">使用子采样的GBDT也称作随机梯度提升树(SGBT)。由于使用了子采样，可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点</span></span></div></li></ul><div>主要缺点有：<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">由于弱学习器之间存在依赖关系，难以并行训练数据</span>。不过可以通过自采样的(Stochastic Gradient Boosting Tree, SGBT)来达到部分并行</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">RF算法</span>作为一个可以高度并行化的算法，数据量大的时候大有可为， 这里也对常规的RF算法的优缺点做一个总结</div><div>RF的主要优点有：</div><ul><li><div>训练可以高度并行化，对于大数据时代的大样本训练速度有优势</div></li><li><div>由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型</div></li><li><div>在训练后，可以给出各个特征对于输出的重要性（<span style="color: rgb(255, 38, 0);">特征选择</span>）</div></li><li><div>由于采用了随机采样，训练出的模型的方差小，泛化能力强</div></li><li><div>相对于Boosting系列的Adaboost和GBDT， RF实现比较简单</div></li><li><div>对部分特征缺失不敏感</div></li></ul><div>RF的主要缺点有：</div><ul><li><div>在某些噪音比较大的样本集上，RF模型容易陷入过拟合</div></li><li><div>取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果</div></li><li><div><br/></div></li></ul><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">为什么随机森林的树深度往往大于 GBDT 的树深度？</span></span></div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(26, 26, 26); font-variant-caps: normal; font-variant-ligatures: normal;">其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias）和方差（variance）。</span><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(170, 121, 66); font-variant-caps: normal; font-variant-ligatures: normal;">偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响</span><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(26, 26, 26); font-variant-caps: normal; font-variant-ligatures: normal;">。</span></font></span></div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(26, 26, 26); font-variant-caps: normal; font-variant-ligatures: normal;">如下图所示：</span></font></span></div><ul><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;" face="Helvetica Neue"><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(255, 38, 0); font-variant-caps: normal; font-variant-ligatures: normal;">当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小</span><span style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; color: rgb(26, 26, 26); font-variant-caps: normal; font-variant-ligatures: normal;">。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。</span></font></span></div></li><li><div><span style="font-size: 14px; orphans: 2; white-space: pre-wrap; widows: 2; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;;">当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。但是因为模型简单，所以偏差会很大。</span></div></li></ul><div style="text-align: center; "><div><img src="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.resources/1D452D3F-7584-40AB-8D5B-917C130C99E3.jpg" height="281" width="455"/><br/></div></div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。</span></span></div><ul><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">对于 Bagging 算法来说，</span><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">由于我们会并行地训练很多不同的分类器的目的就是降低这个方差</span><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">（variance），因为采用了相互独立的基分类器以后，所以</span><span style="background-color: rgb(255, 250, 165); font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;-evernote-highlight:true;">对于每个基分类器来说，目标就是如何降低这个偏差（bias），所以我们会采用深度很深甚至不剪枝的决策树</span><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">。</span></font></span></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">对于 Boosting 来说，每一步都会在上一轮的基础上更加拟合原数据，</span><span style="font-size: 14px; color: rgb(255, 38, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">所以可以保证偏差（bias）</span><span style="font-size: 14px; color: rgb(26, 26, 26); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">，所以对于每个基分类器来说，问题就在于如何选择 variance 更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</span></font></span></div></li></ul><div><br/></div><div>6.怎么理解决策树、xgboost RF能处理缺失值？而有的模型(svm)，线性模型对缺失值比较敏感</div><div><br/></div><div>工具包自动处理数据缺失不代表具体的算法可以处理缺失项。开发者在封装工具库的时候就已经考虑到了,使用者可能导入了含有缺失值的数据，所以加了一个缺失值处理的函数。</div><div>工具包提供自动数据清理的功能的好处：</div><ul><li><div>防止用户导入的数据不符合模型要求而导致失败</div></li><li><div>节省用户的时间，提供一站式服务</div></li></ul><div>工具包提供自动数据清理的功能的风险：</div><ul><li><div>简单粗暴的处理模式会影响模型的结果，自动化的数据清理不可靠</div></li><li><div>用户应该提供符合模型要求的数据，这不是算法工具库的责任。算法工具包的默认要求就是用户提供适合的数据，因为用户对数据有更深刻的理解</div></li><li><div>可能会大幅度增加模型的运算时间</div></li></ul><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">随机森林处理缺失值</span>：</div><ul><li><div>方法1： 快速简单但效果差。把数值型变量中的缺失值用其所对应的类别中(class)的中位数(median)替换。</div></li><li><div>方法2：耗时费力但效果好。虽然依然是使用中位数和出现次数最多的数来进行替换，这里<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">引入权重</span>。即对需要替换的数据先和其他数据做相似度测量也就是下面公式中的Weight，在补全缺失点是相似的点的数据会有更高的权重W。</div></li></ul><div>随机森林填充的优点：</div><ul><li><div>随机森林填补通过构造多棵决策树对缺失值进行填补，使得填补得到的数据具有随机性和不确定性，更能反映出这些未知数据的真实分布；</div></li><li><div>随机森林填补由于在构造决策树过程中，每个分支节点选用随机的部分特征而不是全部特征，所以能很好的应用到高维数据的填补；</div></li><li><div>随机森林算法本身就具有很好的分类精度，从而也更进一步确保了得到的填补值的准确性和可靠性</div></li></ul><div><br/></div><div>XGBoost怎么处理缺失值：把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。</div><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; font-size: 14px;"><a style="font-size: 14px; font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal; line-height: 32px;" href="https://www.zhihu.com/question/58230411">怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?</a></span></div><div><a style="font-size: 14px; orphans: 2; widows: 2; color: rgba(0, 0, 0, 0.901961); font-family: &quot;Helvetica Neue&quot;;" href="http://blog.xiaoduoai.com/?p=278">基于随机森林的缺</a><a style="font-size: 14px; orphans: 2; widows: 2; color: rgba(0, 0, 0, 0.901961); font-family: &quot;Helvetica Neue&quot;;" href="http://blog.xiaoduoai.com/?p=278">失值填补</a></div><div><br/></div><div>7.<span style="color: rgb(255, 38, 0);">什么样的模型对缺失值更敏感</span>？</div><div><span style="color: rgb(255, 38, 0);">对于有缺失的数据：以决策树为原型的模型优于依赖距离度量的模型</span></div><ul><li><div>树模型对于缺失值的敏感度较低，大部分时候可以在数据有缺失时使用</div></li><li><div>涉及到距离度量时，如计算两个点之间的距离，缺失数据就变得比较重要。因为涉及到“距离”这个概念，那么缺失值处理不当就会导致效果很差，如如KNN和SVM</div></li><li><div>线性模型的代价函数(loss function)往往涉及到距离的计算，计算预测值和真实值之间的差别，这容易导致对缺失值敏感</div></li><li><div>神经网络的鲁棒性强，对于缺失数据不是非常敏感，但一般没有那么多数据可供使用。</div></li><li><div>贝叶斯模型对于缺失数据也比较稳定，数据量很小的时候首推贝叶斯模型。</div></li></ul><div><br/></div><div>   总结来看，对于有缺失值的数据在经过缺失值处理后：</div><ul><li><div>数据量很小，用朴素贝叶斯</div></li><li><div> 数据量适中或者较大，用树模型，优先 xgboost</div></li><li><div>数据量较大，也可以用神经网络</div></li><li><div>避免使用距离度量相关的模型，如KNN和SVM</div></li></ul><div><br/></div><div>8. 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</div><div><br/></div><div>bagging方法Bootstrap每次约有<span style="color: rgb(255, 38, 0);">36.8%</span>的样本不会出现在所采集的样本集合中，当然也就没有参加决策树的建立，把这些数据称为袋外数据oob,它可以用于取代测试集误差估计方法。</div><div><br/></div><div><span style="color: rgb(255, 38, 0);">袋外数据(oob)误差的计算方法如下</span>：对于已经生成的随机森林，用袋外数据测试其性能。假设袋外数据总数为O，用这O个袋外数据作为输入，带进之前已经生成的随机森林分类器，分类器会给出O个数据相应的分类，因为这O条数据的类型是已知的，则用正确的分类与随机森林分类器的结果进行比较，统计随机森林分类器分类错误的数目，设为X，则袋外数据误差大小error=X/O; 这已经经过证明是无偏估计的，所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</div><div><br/></div><div>9.GBDT的参数， XGBoost的参数， lightGBM的参数</div><ul><li><div>GBDT的参数（sklearn):</div></li><ul><li><div><span style="color: rgb(170, 121, 66);">n_estimators</span>：控制弱学习器的数量</div></li><li><div><span style="color: rgb(255, 38, 0);">max_depth</span>：设置树深度，深度越大可能过拟合</div></li><li><div><span style="color: rgb(255, 38, 0);">max_leaf_nodes</span>：最大叶子节点数</div></li><li><div><span style="color: rgb(170, 121, 66);">learning_rate</span>：更新过程中用到的收缩步长，(0, 1]</div></li><li><div>max_features：划分时考虑的最大特征数，如果特征数非常多，我们可以灵活使用其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</div></li><li><div><span style="color: rgb(255, 38, 0);">min_samples_split</span>：内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于这个值t，则不会继续再尝试选择最优特征来进行划分。</div></li><li><div><span style="color: rgb(170, 121, 66);">min_samples_leaf</span>：叶子节点最少样本数，这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</div></li><li><div>min_weight_fraction_leaf：叶子节点最小的样本权重和，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。</div></li><li><div>min_impurity_split：节点划分最小不纯度，使用 min_impurity_decrease 替代。</div></li><li><div>min_impurity_decrease：如果节点的纯度下降大于了这个阈值，则进行分裂。</div></li><li><div><span style="color: rgb(255, 38, 0);">subsample</span>：采样比例，取值为(0, 1]，注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做 GBDT 的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低，一般在 [0.5, 0.8] 之间。</div></li><li><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">迭代次数</span></div></li></ul><li><div>XGBoost的参数：</div></li><ul><li><div>booster：基学习器类型，gbtree，gblinear 或 dart（增加了 Dropout） ，gbtree 和 dart 使用基于树的模型，而 gblinear 使用线性模型</div></li><li><div>silent：使用 0 会打印更多信息</div></li><li><div>n-thread：运行时线程数</div></li><li><div>sample_type：采样算法</div></li><li style="list-style: none;"><div>normalize_type：标准化算法</div></li><li style="list-style: none;"><div>rate_drop：前置树的丢弃率，有多少比率的树不进入下一个迭代，[0, 1]</div></li><li style="list-style: none;"><div>one_drop：设置为 1 的话每次至少有一棵树被丢弃。</div></li><li><div>skip_drop：跳过丢弃阶段的概率，[0, 1]，非零的 skip_drop 比 rate_drop 和 one_drop 有更高的优先级</div></li></ul><li><div>lightGBM的参数：</div></li><li><div><br/></div></li></ul><div><span style="font-size: 18px;">参考链接：</span></div><ul><li><div><a href="https://blog.csdn.net/xmu_jupiter/article/details/47314927">从集成学习到模型的偏差和方差的理解</a></div></li><li><div><a href="https://www.zhihu.com/question/26760839">为什么说bagging是减少variance，而boosting是减少bias? </a></div></li><li><div><a href="https://www.zhihu.com/question/27068705">机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？</a></div></li><li><div><a href="http://www.cnblogs.com/LeftNotEasy/archive/2010/12/19/mathmatic_in_machine_learning_2_regression_and_bias_variance_trade_off.html">机器学习中的数学(2)-线性回归，偏差、方差权衡 </a></div></li><li><div><a href="http://3ms.huawei.com/km/groups/2952951/blogs/details/5035945">GBDT 梯度提升决策树 - 当前应用最多的机器学习算法 </a></div></li><li><div><a href="https://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/">深入浅出ML之Boosting家族</a></div></li><li><div><a href="https://www.cnblogs.com/pinard/p/6140514.html">梯度提升树(GBDT)原理小结</a></div></li><li><div><a href="http://www.cnblogs.com/pinard/p/6143927.html">scikit-learn 梯度提升树(GBDT)调参小结</a></div></li><li><div><a href="https://cloud.tencent.com/developer/article/1005937">小巧玲珑：机器学习届快刀XGBoost的介绍和使用</a></div></li><li><div><a href="https://blog.csdn.net/Liangjun_Feng/article/details/80142724?spm=5176.9876270.0.0.246b2ef187R">机器学习教程 之 梯度提升方法：GBDT及其扩展模型XGBoost</a></div></li><li><div><a href="https://zhuanlan.zhihu.com/p/33700459">GBDT、XGBoost、LightGBM 的使用及参数调优</a></div></li><li><div><span style="letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><font style="font-size: 14px;"><a style="font-size: 14px; color: rgb(0, 0, 0); font-family: &quot;Helvetica Neue&quot;; font-variant-caps: normal; font-variant-ligatures: normal;" href="https://blog.csdn.net/data_scientist/article/details/79022025">RF、GBDT、XGBoost、lightGBM原理与区别</a></font></span></div></li><li><div><br/></div></li></ul><div><br/></div></body></html>