<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.1.1 (456663)"/><meta name="altitude" content="11"/><meta name="author" content="杨文家"/><meta name="created" content="2018-05-23 18:15:26 +0000"/><meta name="latitude" content="30.19549269816995"/><meta name="longitude" content="120.1920383197912"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-05-23 18:25:26 +0000"/><title>可能的问题-2</title></head><body><ul style="box-sizing: border-box; padding-left: 30px;"><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">GBDT和随机森林有什么区别？相同点是两个都是集成学习，一个是bagging（每棵树是独立的，权重一样，用的是投票表决机制）， 一个是boosting（没颗树是有依赖的，它的权重是不一样的，最后用的加权累积， boosting可以看作是一种加法模型）. 再展开讲。实际验证效果中GBDT是最好的，工业验证过了（很多比赛），比如xgboost， lightGBM</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">有哪些分类模型？分类模型的话，其实在14年有篇paper，已经发了出来，大概有170多种分类方法，有17个大类，神经网络、bagging类、boosting类、SVM类、LR类等。在很多数据集上测试，发现哪些情况下哪些算法好？随机森林是泛化性能最好的</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">XGBoost与GBDT 的区别</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">AdaBoost与GBDT的异同</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">随机森林为何比决策树Bagging(GBDT)集成的训练速度更快</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">为什么随机森林的树深度往往大于 GBDT 的树深度？</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">什么样的模型对缺失值更敏感？</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">GBDT的参数</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">L1与L2区别：</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">避免过拟合的方法：</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">ftrl算法的必要性</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">FFM原理</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">lr 原理是啥</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;"> LR与线性回归的区别与联系</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">LR为何要对特征离散化</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">LR与SVM的区别</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">SVM的优缺点，原理，如何防止过拟合，为啥要转换成对偶</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">朴素贝叶斯和LR的区别是啥</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">朴素贝叶斯的朴素体现在哪里</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">为什么属性独立性假设在实际任务中很难成立，但是朴素贝叶斯依然可以取的很好的效果？</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">常见的生成事模型和判别式模型</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">KNN算法优缺点</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">EM算法，怎么聚类</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">常见聚类算法有哪些，原理和优缺点</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">PCA和LDA原理</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">特征选择的方法</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">为何要经常对数据做归一化， 常用归一化方法，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">数据标准化和归一化的区别</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">prefixSpan:像频繁序列挖掘，模式挖掘，是吧，是机器学习中的一块，他有一些方法。比如prefixSpan他是一种前缀增长，比如我想挖序列ABCD，先挖到A, 然后再挖掘AB， ABC是吧；由A到挖掘AB的过程中，我需要先把A后面的子序列生产一个投影子空间（或者数据库），从这个子空间中挖掘AB。但是呢，像是MLlib中，我通过阅读代码发现，他这个是通过复制的方式，就相当于我生成了一些投影字空间，但这些字空间中的数据是膨胀的、冗余的，她复制了数组的一部分吗，这样的话很耗内存，性能在大数据集合上很差。我借鉴了python中的列表切片的思想（说白了就是两个指针），切片就是记录起始索引和结束的索引，来吧上述过程给优化掉，这样的话相当于原始数据只有一份，我下面都是通过改变索引位置来挖掘，内存开销也显著降低，并行化性能提高5倍以上。</span></div></li><ul style="box-sizing: border-box; padding-left: 30px;"><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">多用户的话，研究的时候发现没有多用户，就自己想去做一个多用户的，和闭合的序列模式，他的应用场景其实更加的广泛，每个用户他可能有自己的模式，群体也是一种模式挖掘，但是推荐之类的，都是偏个性化的，所以我更关注每个用户他自己的模式；所以自己就补充了多用户的实现（就是加了用户ID, 除了这些还实现了闭合，闭合举个例子，比如Ab出现4词， ABCD出现4次，那AB就是被包含在ABCD中，ABCD就是一个闭合序列）</span></div></li><li style="box-sizing: border-box;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">原有的模式prefixSpan性能在大数据集合上很差, 无论是扩展性还是并行化性能。拿公开数据集去测试，像行驶轨迹啊，道路轨迹啊，比如清华大学出租车数据集，微软亚洲研究院出租车数据（通过车的ID来标记用户，亿级别的数据），发现性能有瓶颈，在哪里</span></div></li></ul><li style="box-sizing: border-box; caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51); font-family: &quot;Open Sans&quot;, &quot;Clear Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; text-indent: 0px; text-transform: none; white-space: pre-wrap; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration: none;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px;-en-paragraph:true;">常用推荐算法：发展趋势，有哪些</span></div></li><li style="box-sizing: border-box; caret-color: rgb(51, 51, 51); color: rgb(51, 51, 51); font-family: &quot;Open Sans&quot;, &quot;Clear Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-size: 16px; font-style: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; text-indent: 0px; text-transform: none; white-space: pre-wrap; widows: auto; word-spacing: 0px; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none;"><div style="box-sizing: border-box; -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; margin-top: 1em; margin-bottom: 1em;"><br/></div></li></ul><div/></body></html>