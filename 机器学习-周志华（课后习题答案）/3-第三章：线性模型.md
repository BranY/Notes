---
typora-copy-images-to: ./picture
---

## 习题

### 3.1 

试分析在什么情况下，在以下式子中不比考虑偏置项b

答：线性模型$y=w^tx+b$，两个实例相减得到$y_i−y_0=w^t(x_i−x_0)$以此消除了b。所以可以对训练集每个样本都减去第一个样本，然后对新的样本做线性回归，只需要用模型$y=w^tx$。

### 3.2 

试证明对于参数$w$，对率回归$（logistics回归）$的目标函数（式1）是非凸的，但其对数似然函数（式2）是凸的。

**答** ：如果一个多元函数是凸的，那么它的$Hessian$矩阵是半正定的。
$$
\begin{align*}
& y = \frac{1}{1+e^{-w^Tx+b}} \\
& \frac{dy}{dw} = \frac{xe^{-(w^Tx+b)}}{(1+e^{-(w^Txb)})^2=x(y-y^2)}\\
& \frac{d}{dw^T}(\frac{dy}{dw})=x(1-2y)(\frac{dy}{dw})^T =xx^Ty(y-1)(1-2y)\\
\end{align*}
$$

$xx^T$合同于单位矩阵，所以$xx^T$是半正定矩阵 
y的值域为$(0,1)$,当$y∈(0.5,1)$时，$y(y−1)(1−2y)<0$,导致$\frac{d}{dw^T}(\frac{dy}{dw})$半负定，所以$y=\frac{1}{1+e^{-(w^Tx+b)}}$是非凸的。

概率$p1∈(0,1)$，则$p1(x;β)(1−p1(x;β))≥0$，所以$l(β)=∑m_i=1(−y_iβ^Tx_i+ln(1+e^{β^Tx}))$是凸函数。

### 3.3 

编程实现对率回归，并给出西瓜数据集3.0α上的结果

详细见[code](https://github.com/BranY/Notes/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E%EF%BC%88%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E7%AD%94%E6%A1%88%EF%BC%89/code)  [nbviewer](http://nbviewer.jupyter.org/github/BranY/Notes/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E%EF%BC%88%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98%E7%AD%94%E6%A1%88%EF%BC%89/code/])

### 3.4

选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率。



### 3.5

编程实现线性判别分析，并给出西瓜数据集3.0α上的结果



### 3.6

LDA仅在线性可分数据上能获得理想结果，试设计一个改进方法，使其能较好地用于非线性可分数据。

答：在当前维度线性不可分，可以使用适当的映射方法，使其在更高一维上可分，典型的方法有$KLDA$，可以很好的划分数据。

### 3.7

令码长为9，类别数为4，试给出海明距离意义下理论最优的EOOC二元码并证明之

答：对于$ECOC$二元码，当码长为2n时，至少可以使2n个类别达到最优间隔，他们的海明距离为$2^{(n−1)}$。比如长度为8时，可以的序列为

|      |      |      |      |      |      |      |      |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1    | 1    | 1    | 1    | -1   | -1   | -1   | -1   |
| 1    | 1    | -1   | -1   | 1    | 1    | -1   | -1   |
| 1    | -1   | 1    | -1   | 1    | -1   | 1    | -1   |
| -1   | -1   | -1   | -1   | 1    | 1    | 1    | 1    |
| -1   | -1   | 1    | 1    | -1   | -1   | 1    | 1    |
| -1   | 1    | -1   | 1    | -1   | 1    | -1   | 1    |

其中4,5,6行是对1,2,3行的取反。若分类数为4，一共可能的分类器共有$2^4−2$种(排除了全1和全0)，在码长为8的最优分类器后添加一列没有出现过的分类器，就是码长为9的最优分类器。

### 3.8

$EOOC$编码能起到理想纠错作用的重要条件是：在每一位编码上出错的概率相当且独立。试析多分类任务经ECOC编码后产生的二类分类器满足该条件的可能性及由此产生的影响

答：理论上的$ECOC码$能理想纠错的重要条件是每个码位出错的概率相当，因为如果某个码位的错误率很高，会导致这位始终保持相同的结果，不再有分类作用，这就相当于全0或者全 1的分类器，这点和NFL的前提很像。但由于事实的样本并不一定满足这些条件，所以书中提到了有多种问题依赖的$ECOC$被提出。

### 3.9

使用$OvR$和$MvM$将多分类任务分解为二分类任务求解时，试述为何无需专门针对类别不平衡性进行处理。

答：对于$OvR$，$MvM$来说，由于对每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需要专门处理。以$ECOC$编码为例，每个生成的二分类器会将所有样本分成较为均衡的二类，使类别不平衡的影响减小。当然拆解后仍然可能出现明显的类别不平衡现象，比如一个超级大类和一群小类。

### 3.10

试推出多分类代价敏感学习(仅考虑基于类别的错误分类代价)使用“再缩放”能获得理论最优解的条件。

答：题目提到仅考虑类别分类的误分类代价，那么就默认正确分类的代价为0。 于是得到分类表,(假设为3类)

| 0    | c12  | c13  |
| ---- | ---- | ---- |
| c21  | 0    | c23  |
| c31  | c32  | 0    |

对于二分类而言，将样本为正例的后验概率设为是p,那么预测为正的代价是$(1−p)∗c_{12}$， 预测为负的代价是$p∗c_{21}$。当$(1−p)∗c_{12}≤p∗c_{21}$样本会被预测成正例，因为他的代价更小。当不等式取等号时，得到了最优划分，这个阀值$p_r=\frac{c_{12}}{c_{12}+c_{21}}$，这表示正例与反例的划分比例应该是初始的$\frac{c_{12}}{c_{21}}$倍。假设分类器预设的阀值是po,不考虑代价敏感时，当$\frac{y}{1−y}>\frac{p_o}{1−p_o}$时取正例。当考虑代价敏感，则应该是$\frac{y}{1−y}>\frac{1−p_r}{p_r}∗\frac{p_o}{1−p_o}=\frac{c_{21}}{c_{12}}∗\frac{p_o}{1−p_o}$。 推广到对于多分类，任意两类的最优再缩放系数$t_{ij}=c_{ij}/c_{ji}$ ,然而所有类别的最优缩放系数并不一定能同时满足。当代价表满足下面条件时，能通过再缩放得到最优解。 
设$t_{ij}=w_i/w_j$，则$w_i/w_j=c_{ij}/c_{ji}$对所有$i,j$成立，假设有k类，共$C_{k}^{2}$个等式，此时代价表中$k∗(k−1)$个数，最少只要知道$2∗(k−1)$就能推出整张表。

## 参考链接

[1.周志华《机器学习》课后习题解答系列（一）：目录](http://blog.csdn.net/snoopy_yuan/article/details/62045353)

[2. 机器学习(周志华西瓜书)参考答案总目录](http://blog.csdn.net/icefire_tyh/article/details/52064910)

[3.机器学习(周志华) 参考答案 第三章 线性模型](http://blog.csdn.net/icefire_tyh/article/details/52069025)

