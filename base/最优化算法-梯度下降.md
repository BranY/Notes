---
typora-copy-images-to: picture
---

# 梯度下降算法

　梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。**梯度下降法的搜索迭代示意图如下图所示：

![350px-Gradient_descent](D:\work\Notes\base\picture\350px-Gradient_descent.png)

牛顿法的缺点：

- **靠近极小值时收敛速度减慢** (如下图)
- **直线搜索时可能会产生一些问题**
- **可能会“之字形”地下降**

![Banana-SteepDesc](D:\work\Notes\base\picture\Banana-SteepDesc.gif)

从上图可以看出，梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。以线性回归算法来对三种梯度下降法进行比较。

比如对一个线性回归$（Linear Logistics）$模型，假设下面的$h(x)$是要拟合的函数，$J(\theta)$为损失函数，$\theta$是要迭代求解的c参数值，$\theta$求解后，那最终要拟合的函数$h(\theta)$就出来了，其中$m$是训练集的样本个数，$n$是特征的个数。
$$
\begin{equation}
\begin{aligned}
\text h(\theta) = \sum_{j=0}^{n}\,\theta_jx_j
\end{aligned}
\end{equation}
$$
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\,(y^i \,-\,h_\theta(x^i))^2
$$

## BGD (Batch Gradient Descent)

批量梯度下降法$（Batch Gradient Descent, BGD）$是梯度下降法最原始的形式，它的具体思路是在**更新每一参数时都使用所有的样本来进行更新**，对损失函数关于$\theta_j$求偏导：
$$
\frac{\partial J(\theta)}{\partial \theta_j}=-\frac{1}{m}\sum_{i=1}^{m}\,(y^i\,-\,h_\theta(x^i))x_j^i
$$
由于是要最小化风险函数，所以按每个参数$\theta$的梯度负方向，来更新每个$\theta$:
$$
\theta_{j}^{'}=\theta_{j}\,+\,\frac{1}{m}\sum_{i=1}^{m}\,(y^i\,-\,h_\theta(x^i))x_j^i
$$
从上面公式可以注意到，它得到的**是一个全局最优解**，但是每迭代一步都要用到训练集所有的数据，如果$m$很大，那么可想而知这种方法的迭代速度会相当的慢。对于批量梯度下降法，样本个数$m$，$x$为$n$维向量，一次迭代需要把$m$个样本全部带入计算，迭代一次计算量为$m*n^2$。

- **优点：**全局最优解；易于并行实现
- **缺点：**当样本数目很多时，训练过程会很慢

从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：

![BGD-example](D:\work\Notes\base\picture\BGD-example.png)

## SGD (Stochastic Gradient Descent)

由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法$（Stochastic Gradient Descent, SGD）$正是为了解决批量梯度下降法这一弊端而提出的。改写损失函数：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\,\frac{1}{2}(y^i\,-\,h_\theta(x^i))^2=\frac{1}{m}\sum_{i=1}^{m}cost(\theta,\,(x_i,y_i))
$$

$$
cost(\theta,\,(x_i,y_i))=\frac{1}{2}(y^i\,-\,h_\theta(x^i))^2
$$

接下来，利用每个样本的损失函数对$theta$求偏导数得到对应的梯度值，来更新$\theta$：
$$
\theta_{j}^{'}=\theta_j + (y^i - h_\theta(x^i))x_{j}^{i}
$$
随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将$\theta$迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代$10$次的话就需要遍历训练样本$10$次。但是$SGD$的缺点是噪音较$BGD$要多，使得$SGD$并不是每次迭代都向着整体最优化方向。

- **优点：**训练速度快
- **缺点：**准确度下降，并不是全局最优，不易于并行实现。

从迭代的次数上来看，$SGD$迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：

![SGD-test](D:\work\Notes\base\picture\SGD-test.png)

### SGD vs BGD

随机梯度下降每次迭代只使用一个样本，迭代一次计算量为$n^2$，当样本个数$m$很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。**两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。**

- **批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下**
- **随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况**

## MBGD (Mini-batch Gradient Descent) 
$SGD$和$BGD$各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法$（Mini-batch Gradient Descent, MBGD）$的初衷。
$MBGD$在每次更新参数时使用$b$个样本（$b$一般为$10$）, $i\in \{1,11,21,31,...,991\}$
$$
\theta_j:=\theta_j-\alpha\frac{1}{10}\sum_{k=i}^{i+9}\,(h_\theta(x^k)-y^k)x_{j}^{k}
$$


