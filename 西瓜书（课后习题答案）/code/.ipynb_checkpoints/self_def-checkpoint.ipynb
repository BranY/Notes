{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def likelihood_sub(x, y, beta):\n",
    "    '''\n",
    "    @param X: one sample variables\n",
    "    @param y: one sample label\n",
    "    @param beta: the parameter vector in 3.27\n",
    "    @return: the sub_log-likelihood of 3.27  \n",
    "    ''' \n",
    "    return -y * np.dot(beta, x.T) + np.math.log(1 + np.math.exp(np.dot(beta, x.T)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(X, y, beta):\n",
    "    '''\n",
    "    @param X: the sample variables matrix\n",
    "    @param y: the sample label matrix\n",
    "    @param beta: the parameter vector in 3.27\n",
    "    @return: the log-likelihood of 3.27  \n",
    "    '''\n",
    "    sum = 0\n",
    "    m,n = np.shape(X)  \n",
    "    \n",
    "    for i in range(m):\n",
    "        sum += likelihood_sub(X[i], y[i], beta)\n",
    "                                                 \n",
    "    return sum       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_derivative(X, y, beta):  # refer to 3.30 on book page 60\n",
    "    '''\n",
    "    @param X: the sample variables matrix\n",
    "    @param y: the sample label matrix\n",
    "    @param beta: the parameter vector in 3.27\n",
    "    @return: the partial derivative of beta [j] \n",
    "    '''\n",
    "\n",
    "    m,n = np.shape(X) \n",
    "    pd = np.zeros(n)\n",
    "    \n",
    "    for i in range(m):\n",
    "        tmp = y[i] - sigmoid(X[i], beta)\n",
    "        for j in range(n):\n",
    "            pd[j] += X[i][j] * (tmp)                                           \n",
    "    return pd   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDscent_1(X, y):  #implementation of fundational gradDscent algorithms\n",
    "    '''\n",
    "    @param X: X is the variable matrix \n",
    "    @param y: y is the label array\n",
    "    @return: the best parameter estimate of 3.27\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt  \n",
    "\n",
    "    h = 0.1  # step length of iterator\n",
    "    max_times= 500  # give the iterative times limit    \n",
    "    m, n = np.shape(X)\n",
    "    \n",
    "    b = np.zeros((n, max_times))  #  for show convergence curve of parameter           \n",
    "    beta = np.zeros(n)  # parameter and initial   \n",
    "    delta_beta = np.ones(n)*h\n",
    "    llh = 0\n",
    "    llh_temp = 0\n",
    "    \n",
    "    for i in range(max_times):\n",
    "        beta_temp = beta\n",
    "        \n",
    "        for j in range(n): \n",
    "            # for partial derivative \n",
    "            beta[j] += delta_beta[j]\n",
    "            llh_tmp = likelihood(X, y, beta)\n",
    "            delta_beta[j] = -h * (llh_tmp - llh) / delta_beta[j]\n",
    "            \n",
    "            b[j,i] = beta[j] \n",
    "            \n",
    "            beta[j] = beta_temp[j]\n",
    "            \n",
    "        beta += delta_beta            \n",
    "        llh = likelihood(X, y, beta)\n",
    "\n",
    "    t = np.arange(max_times)\n",
    "    \n",
    "    f2 = plt.figure(3) \n",
    "    \n",
    "    p1 = plt.subplot(311)\n",
    "    p1.plot(t, b[0])  \n",
    "    plt.ylabel('w1')  \n",
    "    \n",
    "    p2 = plt.subplot(312)\n",
    "    p2.plot(t, b[1])  \n",
    "    plt.ylabel('w2')  \n",
    "        \n",
    "    p3 = plt.subplot(313)\n",
    "    p3.plot(t, b[2])  \n",
    "    plt.ylabel('b')  \n",
    "        \n",
    "    plt.show()               \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDscent_2(X, y):  #implementation of stochastic gradDscent algorithms\n",
    "    '''\n",
    "    @param X: X is the variable matrix \n",
    "    @param y: y is the label array\n",
    "    @return: the best parameter estimate of 3.27\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt  \n",
    "\n",
    "    m, n = np.shape(X)\n",
    "    h = 0.5  #  step length of iterator and initial\n",
    "    beta = np.zeros(n)  # parameter and initial\n",
    "    delta_beta = np.ones(n) * h\n",
    "    llh = 0\n",
    "    llh_temp = 0\n",
    "    b = np.zeros((n, m))  #  for show convergence curve of parameter   \n",
    "\n",
    "    for i in range(m):\n",
    "        beta_temp = beta\n",
    "        \n",
    "        for j in range(n): \n",
    "            # for partial derivative \n",
    "            h = 0.5 * 1 / (1 + i + j)  # change step length of iterator \n",
    "            beta[j] += delta_beta[j]\n",
    "            \n",
    "            b[j,i] = beta[j]\n",
    "            \n",
    "            llh_tmp = likelihood_sub(X[i], y[i], beta)\n",
    "            delta_beta[j] = -h * (llh_tmp - llh) / delta_beta[j]   \n",
    "            \n",
    "            beta[j] = beta_temp[j]  \n",
    "               \n",
    "        beta += delta_beta    \n",
    "        llh = likelihood_sub(X[i], y[i], beta)\n",
    "              \n",
    "    t = np.arange(m)\n",
    "    \n",
    "    f2 = plt.figure(3) \n",
    "    \n",
    "    p1 = plt.subplot(311)\n",
    "    p1.plot(t, b[0])  \n",
    "    plt.ylabel('w1')  \n",
    "    \n",
    "    p2 = plt.subplot(312)\n",
    "    p2.plot(t, b[1])  \n",
    "    plt.ylabel('w2')  \n",
    "        \n",
    "    p3 = plt.subplot(313)\n",
    "    p3.plot(t, b[2])  \n",
    "    plt.ylabel('b')  \n",
    "        \n",
    "    plt.show()   \n",
    "            \n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, beta):\n",
    "    '''\n",
    "    @param x: is the predict variable\n",
    "    @param beta: is the parameter \n",
    "    @return: the sigmoid function value\n",
    "    '''  \n",
    "    return 1.0 / (1 + np.math.exp(- np.dot(beta, x.T))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    '''\n",
    "    prediction the class lable using sigmoid\n",
    "    @param X: data sample form like [x, 1]\n",
    "    @param beta: the parameter of sigmoid form like [w, b]\n",
    "    @return: the class lable array\n",
    "    '''\n",
    "    m, n = np.shape(X)\n",
    "    y = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        if sigmoid(X[i], beta) > 0.5: y[i] = 1;      \n",
    "    return y\n",
    "                            \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
